#!/usr/bin/env python3
"""
FLUX API ååé‡æµ‹è¯•å·¥å…· (ä¼˜åŒ–åˆ†æç‰ˆ)
åˆ†æå½“å‰æ€§èƒ½ç“¶é¢ˆå¹¶æä¾›ä¼˜åŒ–å»ºè®®
"""

import asyncio
import aiohttp
import time
import json
import statistics
import argparse
from typing import List, Dict, Any
from dataclasses import dataclass
from datetime import datetime
import threading
import psutil
import subprocess


@dataclass
class PerformanceMetrics:
    """æ€§èƒ½æŒ‡æ ‡æ•°æ®ç±»"""
    request_id: str
    start_time: float
    end_time: float
    response_time: float
    generation_time: float
    gpu_utilization: float
    vram_usage: float
    cpu_usage: float
    memory_usage: float
    success: bool
    bottleneck: str = ""


class OptimizedFluxTester:
    """ä¼˜åŒ–çš„ FLUX API ååé‡æµ‹è¯•å™¨"""
    
    def __init__(self, base_url: str = "http://localhost:8000"):
        self.base_url = base_url
        self.results: List[PerformanceMetrics] = []
        self.lock = threading.Lock()
        
        # æµ‹è¯•ç”¨çš„æç¤ºè¯
        self.test_prompts = [
            "ä¸€åªå¯çˆ±çš„å°çŒ«åœ¨èŠ±å›­é‡Œç©è€",
            "ç¾ä¸½çš„æ—¥è½é£æ™¯ç”»",
            "æœªæ¥ç§‘æŠ€åŸå¸‚å¤œæ™¯",
            "ä¼ ç»Ÿä¸­å›½å±±æ°´ç”»",
            "ç°ä»£æŠ½è±¡è‰ºæœ¯é£æ ¼",
            "ç«¥è¯æ•…äº‹ä¸­çš„åŸå ¡",
            "æµ·æ´‹æ·±å¤„çš„ç¥ç§˜ç”Ÿç‰©",
            "å¤ªç©ºä¸­çš„æ˜Ÿé™…é£èˆ¹",
            "æ£®æ—ä¸­çš„ç²¾çµä¸–ç•Œ",
            "å¤å¤é£æ ¼çš„è’¸æ±½æœ‹å…‹æœºæ¢°"
        ]
    
    def get_system_metrics(self) -> Dict[str, float]:
        """è·å–ç³»ç»Ÿæ€§èƒ½æŒ‡æ ‡"""
        try:
            # CPU ä½¿ç”¨ç‡
            cpu_percent = psutil.cpu_percent(interval=0.1)
            
            # å†…å­˜ä½¿ç”¨ç‡
            memory = psutil.virtual_memory()
            memory_percent = memory.percent
            
            # GPU ä¿¡æ¯ (å¦‚æœå¯ç”¨)
            gpu_utilization = 0.0
            vram_usage = 0.0
            
            try:
                # å°è¯•è·å– nvidia-smi ä¿¡æ¯
                result = subprocess.run(
                    ['nvidia-smi', '--query-gpu=utilization.gpu,memory.used,memory.total', 
                     '--format=csv,noheader,nounits'],
                    capture_output=True, text=True, timeout=5
                )
                if result.returncode == 0:
                    lines = result.stdout.strip().split('\n')
                    if lines:
                        gpu_info = lines[0].split(', ')
                        if len(gpu_info) >= 3:
                            gpu_utilization = float(gpu_info[0])
                            vram_used = float(gpu_info[1])
                            vram_total = float(gpu_info[2])
                            vram_usage = (vram_used / vram_total) * 100 if vram_total > 0 else 0
            except Exception:
                pass
            
            return {
                "cpu_percent": cpu_percent,
                "memory_percent": memory_percent,
                "gpu_utilization": gpu_utilization,
                "vram_usage": vram_usage
            }
        except Exception:
            return {
                "cpu_percent": 0.0,
                "memory_percent": 0.0,
                "gpu_utilization": 0.0,
                "vram_usage": 0.0
            }
    
    async def test_single_request_with_metrics(self, session: aiohttp.ClientSession, 
                                             prompt: str, request_id: str) -> PerformanceMetrics:
        """æµ‹è¯•å•ä¸ªè¯·æ±‚å¹¶æ”¶é›†æ€§èƒ½æŒ‡æ ‡"""
        start_time = time.time()
        
        # è·å–è¯·æ±‚å‰ç³»ç»ŸæŒ‡æ ‡
        pre_metrics = self.get_system_metrics()
        
        payload = {
            "prompt": prompt,
            "num_inference_steps": 25,
            "guidance_scale": 3.5,
            "width": 512,
            "height": 512
        }
        
        try:
            async with session.post(
                f"{self.base_url}/generate",
                json=payload,
                timeout=aiohttp.ClientTimeout(total=300)
            ) as response:
                end_time = time.time()
                response_time = end_time - start_time
                
                # è·å–è¯·æ±‚åç³»ç»ŸæŒ‡æ ‡
                post_metrics = self.get_system_metrics()
                
                if response.status == 200:
                    result_data = await response.json()
                    generation_time = float(result_data.get("generation_time", "0").replace("s", ""))
                    
                    # åˆ†æç“¶é¢ˆ
                    bottleneck = self.analyze_bottleneck(response_time, generation_time, 
                                                       pre_metrics, post_metrics)
                    
                    return PerformanceMetrics(
                        request_id=request_id,
                        start_time=start_time,
                        end_time=end_time,
                        response_time=response_time,
                        generation_time=generation_time,
                        gpu_utilization=post_metrics["gpu_utilization"],
                        vram_usage=post_metrics["vram_usage"],
                        cpu_usage=post_metrics["cpu_percent"],
                        memory_usage=post_metrics["memory_percent"],
                        success=True,
                        bottleneck=bottleneck
                    )
                else:
                    error_text = await response.text()
                    return PerformanceMetrics(
                        request_id=request_id,
                        start_time=start_time,
                        end_time=end_time,
                        response_time=response_time,
                        generation_time=0.0,
                        gpu_utilization=0.0,
                        vram_usage=0.0,
                        cpu_usage=0.0,
                        memory_usage=0.0,
                        success=False,
                        bottleneck="HTTP_ERROR"
                    )
                    
        except Exception as e:
            end_time = time.time()
            return PerformanceMetrics(
                request_id=request_id,
                start_time=start_time,
                end_time=end_time,
                response_time=end_time - start_time,
                generation_time=0.0,
                gpu_utilization=0.0,
                vram_usage=0.0,
                cpu_usage=0.0,
                memory_usage=0.0,
                success=False,
                bottleneck="EXCEPTION"
            )
    
    def analyze_bottleneck(self, response_time: float, generation_time: float,
                          pre_metrics: Dict[str, float], post_metrics: Dict[str, float]) -> str:
        """åˆ†ææ€§èƒ½ç“¶é¢ˆ"""
        overhead_time = response_time - generation_time
        
        # åˆ†æä¸åŒç±»å‹çš„ç“¶é¢ˆ
        if overhead_time > generation_time * 0.5:
            return "MODEL_LOCK_OVERHEAD"
        elif post_metrics["gpu_utilization"] < 50:
            return "GPU_UNDERUTILIZATION"
        elif post_metrics["vram_usage"] > 90:
            return "VRAM_LIMITATION"
        elif post_metrics["cpu_percent"] > 80:
            return "CPU_BOTTLENECK"
        elif generation_time > 30:
            return "SLOW_INFERENCE"
        else:
            return "OPTIMAL"
    
    async def test_concurrent_requests(self, num_requests: int, max_concurrent: int = 3) -> Dict[str, Any]:
        """æµ‹è¯•å¹¶å‘è¯·æ±‚"""
        print(f"ğŸš€ å¼€å§‹ä¼˜åŒ–åˆ†ææµ‹è¯•: {num_requests} ä¸ªè¯·æ±‚, æœ€å¤§å¹¶å‘æ•°: {max_concurrent}")
        
        # å‡†å¤‡è¯·æ±‚æ•°æ®
        requests_data = []
        for i in range(num_requests):
            prompt = self.test_prompts[i % len(self.test_prompts)]
            requests_data.append((prompt, f"req_{i:04d}"))
        
        # åˆ›å»ºä¿¡å·é‡é™åˆ¶å¹¶å‘æ•°
        semaphore = asyncio.Semaphore(max_concurrent)
        
        async def limited_request(session, prompt, request_id):
            async with semaphore:
                return await self.test_single_request_with_metrics(session, prompt, request_id)
        
        # æ‰§è¡Œå¹¶å‘æµ‹è¯•
        start_time = time.time()
        async with aiohttp.ClientSession() as session:
            tasks = [
                limited_request(session, prompt, request_id)
                for prompt, request_id in requests_data
            ]
            results = await asyncio.gather(*tasks, return_exceptions=True)
        
        end_time = time.time()
        total_time = end_time - start_time
        
        # å¤„ç†ç»“æœ
        with self.lock:
            self.results.extend([r for r in results if isinstance(r, PerformanceMetrics)])
        
        return self.analyze_performance_results(total_time, num_requests)
    
    def analyze_performance_results(self, total_time: float, num_requests: int) -> Dict[str, Any]:
        """åˆ†ææ€§èƒ½ç»“æœ"""
        successful_results = [r for r in self.results if r.success]
        failed_results = [r for r in self.results if not r.success]
        
        if not successful_results:
            return {
                "total_requests": num_requests,
                "successful_requests": 0,
                "failed_requests": len(failed_results),
                "success_rate": 0.0,
                "total_time": total_time,
                "throughput": 0.0,
                "error": "æ²¡æœ‰æˆåŠŸçš„è¯·æ±‚"
            }
        
        # åŸºç¡€ç»Ÿè®¡
        response_times = [r.response_time for r in successful_results]
        generation_times = [r.generation_time for r in successful_results]
        
        # ç“¶é¢ˆåˆ†æ
        bottlenecks = [r.bottleneck for r in successful_results]
        bottleneck_counts = {}
        for bottleneck in bottlenecks:
            bottleneck_counts[bottleneck] = bottleneck_counts.get(bottleneck, 0) + 1
        
        # èµ„æºä½¿ç”¨åˆ†æ
        avg_gpu_utilization = statistics.mean([r.gpu_utilization for r in successful_results])
        avg_vram_usage = statistics.mean([r.vram_usage for r in successful_results])
        avg_cpu_usage = statistics.mean([r.cpu_usage for r in successful_results])
        avg_memory_usage = statistics.mean([r.memory_usage for r in successful_results])
        
        analysis = {
            "total_requests": num_requests,
            "successful_requests": len(successful_results),
            "failed_requests": len(failed_results),
            "success_rate": len(successful_results) / num_requests * 100,
            "total_time": total_time,
            "throughput": len(successful_results) / total_time,
            
            # æ—¶é—´ç»Ÿè®¡
            "avg_response_time": statistics.mean(response_times),
            "median_response_time": statistics.median(response_times),
            "min_response_time": min(response_times),
            "max_response_time": max(response_times),
            "response_time_std": statistics.stdev(response_times) if len(response_times) > 1 else 0,
            
            "avg_generation_time": statistics.mean(generation_times),
            "median_generation_time": statistics.median(generation_times),
            
            # ç“¶é¢ˆåˆ†æ
            "bottleneck_analysis": bottleneck_counts,
            "main_bottleneck": max(bottleneck_counts.items(), key=lambda x: x[1])[0] if bottleneck_counts else "UNKNOWN",
            
            # èµ„æºä½¿ç”¨
            "avg_gpu_utilization": avg_gpu_utilization,
            "avg_vram_usage": avg_vram_usage,
            "avg_cpu_usage": avg_cpu_usage,
            "avg_memory_usage": avg_memory_usage,
            
            # ä¼˜åŒ–å»ºè®®
            "optimization_suggestions": self.generate_optimization_suggestions(
                bottleneck_counts, avg_gpu_utilization, avg_vram_usage, avg_cpu_usage
            )
        }
        
        return analysis
    
    def generate_optimization_suggestions(self, bottleneck_counts: Dict[str, int], 
                                        gpu_utilization: float, vram_usage: float, 
                                        cpu_usage: float) -> List[str]:
        """ç”Ÿæˆä¼˜åŒ–å»ºè®®"""
        suggestions = []
        
        # åŸºäºç“¶é¢ˆåˆ†æçš„å»ºè®®
        if "MODEL_LOCK_OVERHEAD" in bottleneck_counts:
            suggestions.append("ğŸ”§ å®ç°æ¨¡å‹æ± æ¨¡å¼ï¼Œå‡å°‘æ¨¡å‹é”ç«äº‰")
            suggestions.append("ğŸ”§ åˆ†ç¦»æ¨¡å‹åŠ è½½å’Œæ¨ç†çš„é”æœºåˆ¶")
            suggestions.append("ğŸ”§ ä½¿ç”¨å¼‚æ­¥æ¨¡å‹è®¿é—®æ¨¡å¼")
        
        if "GPU_UNDERUTILIZATION" in bottleneck_counts:
            suggestions.append("ğŸš€ å¢åŠ å¹¶å‘è¯·æ±‚æ•°é‡")
            suggestions.append("ğŸš€ å®ç°æ‰¹é‡æ¨ç† (Batch Inference)")
            suggestions.append("ğŸš€ å¯ç”¨ torch.compile() ä¼˜åŒ–")
        
        if "VRAM_LIMITATION" in bottleneck_counts:
            suggestions.append("ğŸ’¾ ä¼˜åŒ–å†…å­˜ç®¡ç†ï¼Œå®ç°å†…å­˜æ± ")
            suggestions.append("ğŸ’¾ ä½¿ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹å‡å°‘å†…å­˜å ç”¨")
            suggestions.append("ğŸ’¾ è€ƒè™‘ä½¿ç”¨æ›´å°çš„æ¨¡å‹æˆ–é‡åŒ–")
        
        if "CPU_BOTTLENECK" in bottleneck_counts:
            suggestions.append("âš¡ ä¼˜åŒ– CPU å¯†é›†å‹æ“ä½œ")
            suggestions.append("âš¡ ä½¿ç”¨å¼‚æ­¥ I/O å¤„ç†")
            suggestions.append("âš¡ å‡å°‘ä¸å¿…è¦çš„ CPU è®¡ç®—")
        
        if "SLOW_INFERENCE" in bottleneck_counts:
            suggestions.append("ğŸ¯ å‡å°‘æ¨ç†æ­¥æ•° (num_inference_steps)")
            suggestions.append("ğŸ¯ ä½¿ç”¨æ›´å¿«çš„æ¨¡å‹å˜ä½“")
            suggestions.append("ğŸ¯ å¯ç”¨ CUDA Graph ä¼˜åŒ–")
        
        # åŸºäºèµ„æºä½¿ç”¨çš„å»ºè®®
        if gpu_utilization < 50:
            suggestions.append("ğŸ“ˆ GPU åˆ©ç”¨ç‡ä½ï¼Œå¯ä»¥å¢åŠ å¹¶å‘æ•°")
        
        if vram_usage > 80:
            suggestions.append("âš ï¸ VRAM ä½¿ç”¨ç‡é«˜ï¼Œè€ƒè™‘å†…å­˜ä¼˜åŒ–")
        
        if cpu_usage > 70:
            suggestions.append("âš ï¸ CPU ä½¿ç”¨ç‡é«˜ï¼Œæ£€æŸ¥æ˜¯å¦æœ‰ CPU ç“¶é¢ˆ")
        
        return suggestions
    
    def generate_optimization_report(self, test_name: str, results: Dict[str, Any]):
        """ç”Ÿæˆä¼˜åŒ–åˆ†ææŠ¥å‘Š"""
        print(f"\n{'='*80}")
        print(f"ğŸ“Š {test_name} ä¼˜åŒ–åˆ†ææŠ¥å‘Š")
        print(f"{'='*80}")
        
        print(f"ğŸ“ˆ åŸºç¡€æ€§èƒ½æŒ‡æ ‡:")
        print(f"   æ€»è¯·æ±‚æ•°: {results['total_requests']}")
        print(f"   æˆåŠŸè¯·æ±‚: {results['successful_requests']}")
        print(f"   æˆåŠŸç‡: {results['success_rate']:.2f}%")
        print(f"   æ€»è€—æ—¶: {results['total_time']:.2f}ç§’")
        print(f"   ååé‡: {results['throughput']:.2f} è¯·æ±‚/ç§’")
        
        print(f"\nâ±ï¸  æ—¶é—´åˆ†æ:")
        print(f"   å¹³å‡å“åº”æ—¶é—´: {results['avg_response_time']:.2f}ç§’")
        print(f"   å¹³å‡ç”Ÿæˆæ—¶é—´: {results['avg_generation_time']:.2f}ç§’")
        print(f"   å“åº”æ—¶é—´æ ‡å‡†å·®: {results['response_time_std']:.2f}ç§’")
        
        print(f"\nğŸ” ç“¶é¢ˆåˆ†æ:")
        for bottleneck, count in results['bottleneck_analysis'].items():
            percentage = (count / results['successful_requests']) * 100
            print(f"   {bottleneck}: {count} æ¬¡ ({percentage:.1f}%)")
        
        print(f"\nğŸ’» èµ„æºä½¿ç”¨æƒ…å†µ:")
        print(f"   å¹³å‡ GPU åˆ©ç”¨ç‡: {results['avg_gpu_utilization']:.1f}%")
        print(f"   å¹³å‡ VRAM ä½¿ç”¨ç‡: {results['avg_vram_usage']:.1f}%")
        print(f"   å¹³å‡ CPU ä½¿ç”¨ç‡: {results['avg_cpu_usage']:.1f}%")
        print(f"   å¹³å‡å†…å­˜ä½¿ç”¨ç‡: {results['avg_memory_usage']:.1f}%")
        
        print(f"\nğŸ¯ ä¸»è¦ç“¶é¢ˆ: {results['main_bottleneck']}")
        
        print(f"\nğŸ’¡ ä¼˜åŒ–å»ºè®®:")
        for i, suggestion in enumerate(results['optimization_suggestions'], 1):
            print(f"   {i}. {suggestion}")
        
        print(f"{'='*80}\n")


async def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="FLUX API ååé‡ä¼˜åŒ–åˆ†æå·¥å…·")
    parser.add_argument("--url", default="http://localhost:8000", help="API åŸºç¡€URL")
    parser.add_argument("--requests", type=int, default=20, help="æµ‹è¯•è¯·æ±‚æ•°é‡")
    parser.add_argument("--concurrent", type=int, default=3, help="æœ€å¤§å¹¶å‘æ•°")
    parser.add_argument("--save", action="store_true", help="ä¿å­˜è¯¦ç»†ç»“æœåˆ°JSONæ–‡ä»¶")
    
    args = parser.parse_args()
    
    print("ğŸš€ FLUX API ååé‡ä¼˜åŒ–åˆ†æå·¥å…·")
    print(f"ğŸ“ API URL: {args.url}")
    print(f"ğŸ“Š æµ‹è¯•é…ç½®: {args.requests} è¯·æ±‚, {args.concurrent} å¹¶å‘")
    
    tester = OptimizedFluxTester(args.url)
    
    # æ‰§è¡Œæµ‹è¯•
    print(f"\nğŸ”„ å¼€å§‹æ€§èƒ½åˆ†ææµ‹è¯•...")
    results = await tester.test_concurrent_requests(args.requests, args.concurrent)
    tester.generate_optimization_report("æ€§èƒ½ä¼˜åŒ–åˆ†æ", results)
    
    # ä¿å­˜è¯¦ç»†ç»“æœ
    if args.save:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        results_file = f"optimization_analysis_{timestamp}.json"
        
        detailed_results = {
            "test_config": {
                "url": args.url,
                "requests": args.requests,
                "concurrent": args.concurrent
            },
            "performance_analysis": results,
            "detailed_metrics": [
                {
                    "request_id": r.request_id,
                    "response_time": r.response_time,
                    "generation_time": r.generation_time,
                    "gpu_utilization": r.gpu_utilization,
                    "vram_usage": r.vram_usage,
                    "cpu_usage": r.cpu_usage,
                    "memory_usage": r.memory_usage,
                    "bottleneck": r.bottleneck,
                    "success": r.success
                }
                for r in tester.results
            ]
        }
        
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(detailed_results, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ“„ è¯¦ç»†åˆ†æç»“æœå·²ä¿å­˜åˆ°: {results_file}")


if __name__ == "__main__":
    asyncio.run(main()) 