#!/usr/bin/env python3
"""
FLUX API ååé‡æµ‹è¯•å·¥å…·
æµ‹è¯• FP4 æ¨¡å‹çš„å¹¶å‘å¤„ç†èƒ½åŠ›å’Œå“åº”æ—¶é—´
"""

import asyncio
import aiohttp
import time
import json
import statistics
import argparse
from typing import List, Dict, Any
from dataclasses import dataclass
from datetime import datetime
import matplotlib.pyplot as plt
import numpy as np
from concurrent.futures import ThreadPoolExecutor
import threading


@dataclass
class TestResult:
    """æµ‹è¯•ç»“æœæ•°æ®ç±»"""
    request_id: str
    start_time: float
    end_time: float
    response_time: float
    status_code: int
    success: bool
    error_message: str = ""
    generation_time: float = 0.0
    vram_usage: str = ""
    image_size: str = ""


class FluxThroughputTester:
    """FLUX API ååé‡æµ‹è¯•å™¨"""
    
    def __init__(self, base_url: str = "http://localhost:8000"):
        self.base_url = base_url
        self.results: List[TestResult] = []
        self.lock = threading.Lock()
        
        # æµ‹è¯•ç”¨çš„æç¤ºè¯æ¨¡æ¿
        self.test_prompts = [
            "ä¸€åªå¯çˆ±çš„å°çŒ«åœ¨èŠ±å›­é‡Œç©è€",
            "ç¾ä¸½çš„æ—¥è½é£æ™¯ç”»",
            "æœªæ¥ç§‘æŠ€åŸå¸‚å¤œæ™¯",
            "ä¼ ç»Ÿä¸­å›½å±±æ°´ç”»",
            "ç°ä»£æŠ½è±¡è‰ºæœ¯é£æ ¼",
            "ç«¥è¯æ•…äº‹ä¸­çš„åŸå ¡",
            "æµ·æ´‹æ·±å¤„çš„ç¥ç§˜ç”Ÿç‰©",
            "å¤ªç©ºä¸­çš„æ˜Ÿé™…é£èˆ¹",
            "æ£®æ—ä¸­çš„ç²¾çµä¸–ç•Œ",
            "å¤å¤é£æ ¼çš„è’¸æ±½æœ‹å…‹æœºæ¢°"
        ]
    
    async def test_single_request(self, session: aiohttp.ClientSession, prompt: str, 
                                request_id: str) -> TestResult:
        """æµ‹è¯•å•ä¸ªè¯·æ±‚"""
        start_time = time.time()
        
        payload = {
            "prompt": prompt,
            "num_inference_steps": 25,
            "guidance_scale": 3.5,
            "width": 512,
            "height": 512
        }
        
        try:
            async with session.post(
                f"{self.base_url}/generate",
                json=payload,
                timeout=aiohttp.ClientTimeout(total=300)  # 5åˆ†é’Ÿè¶…æ—¶
            ) as response:
                end_time = time.time()
                response_time = end_time - start_time
                
                if response.status == 200:
                    result_data = await response.json()
                    generation_time = float(result_data.get("generation_time", "0").replace("s", ""))
                    vram_usage = result_data.get("vram_usage_gb", "")
                    
                    return TestResult(
                        request_id=request_id,
                        start_time=start_time,
                        end_time=end_time,
                        response_time=response_time,
                        status_code=response.status,
                        success=True,
                        generation_time=generation_time,
                        vram_usage=vram_usage,
                        image_size=f"{payload['width']}x{payload['height']}"
                    )
                else:
                    error_text = await response.text()
                    return TestResult(
                        request_id=request_id,
                        start_time=start_time,
                        end_time=end_time,
                        response_time=response_time,
                        status_code=response.status,
                        success=False,
                        error_message=error_text
                    )
                    
        except Exception as e:
            end_time = time.time()
            return TestResult(
                request_id=request_id,
                start_time=start_time,
                end_time=end_time,
                response_time=end_time - start_time,
                status_code=0,
                success=False,
                error_message=str(e)
            )
    
    async def test_concurrent_requests(self, num_requests: int, max_concurrent: int = 5) -> Dict[str, Any]:
        """æµ‹è¯•å¹¶å‘è¯·æ±‚"""
        print(f"ğŸš€ å¼€å§‹å¹¶å‘æµ‹è¯•: {num_requests} ä¸ªè¯·æ±‚, æœ€å¤§å¹¶å‘æ•°: {max_concurrent}")
        
        # å‡†å¤‡è¯·æ±‚æ•°æ®
        requests_data = []
        for i in range(num_requests):
            prompt = self.test_prompts[i % len(self.test_prompts)]
            requests_data.append((prompt, f"req_{i:04d}"))
        
        # åˆ›å»ºä¿¡å·é‡é™åˆ¶å¹¶å‘æ•°
        semaphore = asyncio.Semaphore(max_concurrent)
        
        async def limited_request(session, prompt, request_id):
            async with semaphore:
                return await self.test_single_request(session, prompt, request_id)
        
        # æ‰§è¡Œå¹¶å‘æµ‹è¯•
        start_time = time.time()
        async with aiohttp.ClientSession() as session:
            tasks = [
                limited_request(session, prompt, request_id)
                for prompt, request_id in requests_data
            ]
            results = await asyncio.gather(*tasks, return_exceptions=True)
        
        end_time = time.time()
        total_time = end_time - start_time
        
        # å¤„ç†ç»“æœ
        with self.lock:
            self.results.extend([r for r in results if isinstance(r, TestResult)])
        
        return self.analyze_results(total_time, num_requests)
    

    
    async def test_model_status(self) -> Dict[str, Any]:
        """æµ‹è¯•æ¨¡å‹çŠ¶æ€å’Œç³»ç»Ÿä¿¡æ¯"""
        print("ğŸ” æ£€æŸ¥æ¨¡å‹çŠ¶æ€...")
        
        async with aiohttp.ClientSession() as session:
            # æ£€æŸ¥æ¨¡å‹çŠ¶æ€
            async with session.get(f"{self.base_url}/model-status") as response:
                if response.status == 200:
                    model_status = await response.json()
                else:
                    model_status = {"error": "æ— æ³•è·å–æ¨¡å‹çŠ¶æ€"}
            
            # æ£€æŸ¥GPUä¿¡æ¯
            async with session.get(f"{self.base_url}/gpu-info") as response:
                if response.status == 200:
                    gpu_info = await response.json()
                else:
                    gpu_info = {"error": "æ— æ³•è·å–GPUä¿¡æ¯"}
            
            # æ£€æŸ¥é˜Ÿåˆ—ç»Ÿè®¡
            async with session.get(f"{self.base_url}/queue-stats") as response:
                if response.status == 200:
                    queue_stats = await response.json()
                else:
                    queue_stats = {"error": "æ— æ³•è·å–é˜Ÿåˆ—ç»Ÿè®¡"}
            
            return {
                "model_status": model_status,
                "gpu_info": gpu_info,
                "queue_stats": queue_stats,
                "timestamp": datetime.now().isoformat()
            }
    
    def analyze_results(self, total_time: float, num_requests: int) -> Dict[str, Any]:
        """åˆ†ææµ‹è¯•ç»“æœ"""
        successful_results = [r for r in self.results if r.success]
        failed_results = [r for r in self.results if not r.success]
        
        if not successful_results:
            return {
                "total_requests": num_requests,
                "successful_requests": 0,
                "failed_requests": len(failed_results),
                "success_rate": 0.0,
                "total_time": total_time,
                "throughput": 0.0,
                "error": "æ²¡æœ‰æˆåŠŸçš„è¯·æ±‚"
            }
        
        response_times = [r.response_time for r in successful_results]
        generation_times = [r.generation_time for r in successful_results if r.generation_time > 0]
        
        analysis = {
            "total_requests": num_requests,
            "successful_requests": len(successful_results),
            "failed_requests": len(failed_results),
            "success_rate": len(successful_results) / num_requests * 100,
            "total_time": total_time,
            "throughput": len(successful_results) / total_time,  # è¯·æ±‚/ç§’
            "avg_response_time": statistics.mean(response_times),
            "median_response_time": statistics.median(response_times),
            "min_response_time": min(response_times),
            "max_response_time": max(response_times),
            "response_time_std": statistics.stdev(response_times) if len(response_times) > 1 else 0,
            "avg_generation_time": statistics.mean(generation_times) if generation_times else 0,
            "median_generation_time": statistics.median(generation_times) if generation_times else 0,
            "errors": [r.error_message for r in failed_results] if failed_results else []
        }
        
        return analysis
    
    def generate_report(self, test_name: str, results: Dict[str, Any]):
        """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"""
        print(f"\n{'='*60}")
        print(f"ğŸ“Š {test_name} æµ‹è¯•æŠ¥å‘Š")
        print(f"{'='*60}")
        
        print(f"ğŸ“ˆ æ€»ä½“ç»Ÿè®¡:")
        print(f"   æ€»è¯·æ±‚æ•°: {results['total_requests']}")
        print(f"   æˆåŠŸè¯·æ±‚: {results['successful_requests']}")
        print(f"   å¤±è´¥è¯·æ±‚: {results['failed_requests']}")
        print(f"   æˆåŠŸç‡: {results['success_rate']:.2f}%")
        print(f"   æ€»è€—æ—¶: {results['total_time']:.2f}ç§’")
        print(f"   ååé‡: {results['throughput']:.2f} è¯·æ±‚/ç§’")
        
        print(f"\nâ±ï¸  å“åº”æ—¶é—´:")
        print(f"   å¹³å‡å“åº”æ—¶é—´: {results['avg_response_time']:.2f}ç§’")
        print(f"   ä¸­ä½æ•°å“åº”æ—¶é—´: {results['median_response_time']:.2f}ç§’")
        print(f"   æœ€å°å“åº”æ—¶é—´: {results['min_response_time']:.2f}ç§’")
        print(f"   æœ€å¤§å“åº”æ—¶é—´: {results['max_response_time']:.2f}ç§’")
        print(f"   å“åº”æ—¶é—´æ ‡å‡†å·®: {results['response_time_std']:.2f}ç§’")
        
        if results.get('avg_generation_time', 0) > 0:
            print(f"\nğŸ¨ ç”Ÿæˆæ—¶é—´:")
            print(f"   å¹³å‡ç”Ÿæˆæ—¶é—´: {results['avg_generation_time']:.2f}ç§’")
            print(f"   ä¸­ä½æ•°ç”Ÿæˆæ—¶é—´: {results['median_generation_time']:.2f}ç§’")
        
        if results.get('errors'):
            print(f"\nâŒ é”™è¯¯ä¿¡æ¯:")
            for error in results['errors'][:5]:  # åªæ˜¾ç¤ºå‰5ä¸ªé”™è¯¯
                print(f"   - {error}")
            if len(results['errors']) > 5:
                print(f"   ... è¿˜æœ‰ {len(results['errors']) - 5} ä¸ªé”™è¯¯")
        
        print(f"{'='*60}\n")
    
    def plot_results(self, save_path: str = "throughput_test_results.png"):
        """ç»˜åˆ¶æµ‹è¯•ç»“æœå›¾è¡¨"""
        if not self.results:
            print("âŒ æ²¡æœ‰æµ‹è¯•ç»“æœå¯ä»¥ç»˜åˆ¶")
            return
        
        successful_results = [r for r in self.results if r.success]
        if not successful_results:
            print("âŒ æ²¡æœ‰æˆåŠŸçš„æµ‹è¯•ç»“æœå¯ä»¥ç»˜åˆ¶")
            return
        
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))
        
        # å“åº”æ—¶é—´åˆ†å¸ƒ
        response_times = [r.response_time for r in successful_results]
        ax1.hist(response_times, bins=20, alpha=0.7, color='skyblue', edgecolor='black')
        ax1.set_xlabel('å“åº”æ—¶é—´ (ç§’)')
        ax1.set_ylabel('è¯·æ±‚æ•°é‡')
        ax1.set_title('å“åº”æ—¶é—´åˆ†å¸ƒ')
        ax1.axvline(statistics.mean(response_times), color='red', linestyle='--', 
                   label=f'å¹³å‡å€¼: {statistics.mean(response_times):.2f}s')
        ax1.legend()
        
        # ç”Ÿæˆæ—¶é—´åˆ†å¸ƒ
        generation_times = [r.generation_time for r in successful_results if r.generation_time > 0]
        if generation_times:
            ax2.hist(generation_times, bins=20, alpha=0.7, color='lightgreen', edgecolor='black')
            ax2.set_xlabel('ç”Ÿæˆæ—¶é—´ (ç§’)')
            ax2.set_ylabel('è¯·æ±‚æ•°é‡')
            ax2.set_title('ç”Ÿæˆæ—¶é—´åˆ†å¸ƒ')
            ax2.axvline(statistics.mean(generation_times), color='red', linestyle='--',
                       label=f'å¹³å‡å€¼: {statistics.mean(generation_times):.2f}s')
            ax2.legend()
        else:
            ax2.text(0.5, 0.5, 'æ— ç”Ÿæˆæ—¶é—´æ•°æ®', ha='center', va='center', transform=ax2.transAxes)
            ax2.set_title('ç”Ÿæˆæ—¶é—´åˆ†å¸ƒ')
        
        # è¯·æ±‚æ—¶é—´çº¿
        start_times = [r.start_time for r in successful_results]
        relative_times = [t - min(start_times) for t in start_times]
        ax3.scatter(relative_times, range(len(relative_times)), alpha=0.6, s=20)
        ax3.set_xlabel('ç›¸å¯¹æ—¶é—´ (ç§’)')
        ax3.set_ylabel('è¯·æ±‚åºå·')
        ax3.set_title('è¯·æ±‚æ—¶é—´çº¿')
        
        # ååé‡è®¡ç®—
        time_windows = np.linspace(0, max(relative_times), 20)
        throughput = []
        for i in range(len(time_windows) - 1):
            count = sum(1 for t in relative_times if time_windows[i] <= t < time_windows[i+1])
            throughput.append(count / (time_windows[i+1] - time_windows[i]))
        
        ax4.plot(time_windows[:-1], throughput, marker='o', alpha=0.7)
        ax4.set_xlabel('æ—¶é—´çª—å£ (ç§’)')
        ax4.set_ylabel('ååé‡ (è¯·æ±‚/ç§’)')
        ax4.set_title('å®æ—¶ååé‡')
        ax4.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"ğŸ“Š å›¾è¡¨å·²ä¿å­˜åˆ°: {save_path}")
        plt.show()


async def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="FLUX API ååé‡æµ‹è¯•å·¥å…·")
    parser.add_argument("--url", default="http://localhost:8000", help="API åŸºç¡€URL")
    parser.add_argument("--requests", type=int, default=10, help="æµ‹è¯•è¯·æ±‚æ•°é‡")
    parser.add_argument("--concurrent", type=int, default=3, help="æœ€å¤§å¹¶å‘æ•°")
    parser.add_argument("--test-type", choices=["concurrent"], 
                       default="concurrent", help="æµ‹è¯•ç±»å‹")
    parser.add_argument("--plot", action="store_true", help="ç”Ÿæˆå›¾è¡¨")
    
    args = parser.parse_args()
    
    print("ğŸš€ FLUX API ååé‡æµ‹è¯•å·¥å…·")
    print(f"ğŸ“ API URL: {args.url}")
    print(f"ğŸ“Š æµ‹è¯•é…ç½®: {args.requests} è¯·æ±‚, {args.concurrent} å¹¶å‘")
    print(f"ğŸ¯ æµ‹è¯•ç±»å‹: {args.test_type}")
    
    tester = FluxThroughputTester(args.url)
    
    # æ£€æŸ¥æ¨¡å‹çŠ¶æ€
    print("\nğŸ” æ£€æŸ¥ç³»ç»ŸçŠ¶æ€...")
    status = await tester.test_model_status()
    print(f"æ¨¡å‹çŠ¶æ€: {status['model_status'].get('model_loaded', 'Unknown')}")
    print(f"GPU ä¿¡æ¯: {status['gpu_info']}")
    print(f"é˜Ÿåˆ—ç»Ÿè®¡: {status['queue_stats']}")
    
    # æ‰§è¡Œæµ‹è¯•
    print(f"\nğŸ”„ å¼€å§‹å¹¶å‘æµ‹è¯•...")
    concurrent_results = await tester.test_concurrent_requests(
        args.requests, args.concurrent
    )
    tester.generate_report("å¹¶å‘è¯·æ±‚", concurrent_results)
    
    # ç”Ÿæˆå›¾è¡¨
    if args.plot:
        tester.plot_results()
    
    # ä¿å­˜è¯¦ç»†ç»“æœ
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    results_file = f"throughput_test_results_{timestamp}.json"
    
    detailed_results = {
        "test_config": {
            "url": args.url,
            "requests": args.requests,
            "concurrent": args.concurrent,
            "test_type": args.test_type
        },
        "system_status": status,
        "test_results": [
            {
                "request_id": r.request_id,
                "start_time": r.start_time,
                "end_time": r.end_time,
                "response_time": r.response_time,
                "status_code": r.status_code,
                "success": r.success,
                "error_message": r.error_message,
                "generation_time": r.generation_time,
                "vram_usage": r.vram_usage,
                "image_size": r.image_size
            }
            for r in tester.results
        ]
    }
    
    with open(results_file, 'w', encoding='utf-8') as f:
        json.dump(detailed_results, f, indent=2, ensure_ascii=False)
    
    print(f"ğŸ“„ è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: {results_file}")


if __name__ == "__main__":
    asyncio.run(main()) 