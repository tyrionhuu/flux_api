services:
  flux-api:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: flux-img2img-api
    restart: unless-stopped
    
    # GPU configuration - enable all GPUs by default
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    
    # Environment variables
    environment:
      - CUDA_VISIBLE_DEVICES=all
      - FP4_API_PORT=9000
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
    
    # Port mapping
    ports:
      - "9000:9000"
    
    # Volume mounts for persistent data
    volumes:
      - ./generated_images:/app/generated_images
      - ./logs:/app/logs
      - ./uploads:/app/uploads
      - ./cache:/app/cache
      - ~/.cache/huggingface:/root/.cache/huggingface
      - ~/.cache/torch:/root/.cache/torch
    
    # Health check
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    
    # Resource limits (adjust based on your system)
    # mem_limit: 32g
    # memswap_limit: 32g
    
    # Security options
    security_opt:
      - seccomp:unconfined
    
    # Shared memory for PyTorch
    shm_size: 2g

  # Optional: Nginx reverse proxy for production
  nginx:
    image: nginx:alpine
    container_name: flux-nginx
    restart: unless-stopped
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf:ro
      - ./ssl:/etc/nginx/ssl:ro
    depends_on:
      - flux-api
    profiles:
      - production

# Networks
networks:
  default:
    name: flux-network
    driver: bridge

# Volumes for persistent data
volumes:
  huggingface_cache:
    driver: local
  torch_cache:
    driver: local
  generated_images:
    driver: local
  logs:
    driver: local
