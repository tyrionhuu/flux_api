services:
  flux:
    build: .
    image: flux-api:latest
    container_name: flux
    command: ["-m", "fp4_sekai"]
    environment:
      # Select which GPUs this service can see (comma-separated indices)
      CUDA_VISIBLE_DEVICES: ${CUDA_VISIBLE_DEVICES:-0,1}
      NVIDIA_VISIBLE_DEVICES: ${CUDA_VISIBLE_DEVICES:-0,1}
      NVIDIA_DRIVER_CAPABILITIES: all
      # Optional tuning for per-instance concurrency/queueing
      MAX_CONCURRENT_REQUESTS: ${MAX_CONCURRENT_REQUESTS:-1}
      REQUEST_TIMEOUT: ${REQUEST_TIMEOUT:-600}
    ports:
      - "8080:8080"               # Nginx load balancer
      - "23333-23340:23333-23340" # Per-GPU instances (optional)
    volumes:
      - ./logs:/app/logs
      - ./generated_images:/app/generated_images
      - ./uploads:/app/uploads
      - ${HOME}/.cache/huggingface:/root/.cache/huggingface
    restart: unless-stopped
    ulimits:
      nofile: 65535
    healthcheck:
      test: ["CMD-SHELL", "curl -sf http://localhost:8080/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 10
      start_period: 60s
    # GPU reservation (ignored by vanilla Compose but works with Swarm/modern engines)
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: ${GPU_COUNT:-2}
              capabilities: ["gpu"]
