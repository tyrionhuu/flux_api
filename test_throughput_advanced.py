#!/usr/bin/env python3
"""
FLUX API é«˜çº§ååé‡æµ‹è¯•å·¥å…·
æµ‹è¯•ä¸åŒä¼˜åŒ–ç­–ç•¥å¯¹æ€§èƒ½çš„å½±å“
"""

import asyncio
import aiohttp
import time
import json
import statistics
import argparse
from typing import List, Dict, Any, Optional
from dataclasses import dataclass
from datetime import datetime
import threading
import psutil
import subprocess
import numpy as np
from concurrent.futures import ThreadPoolExecutor


@dataclass
class AdvancedTestResult:
    """é«˜çº§æµ‹è¯•ç»“æœæ•°æ®ç±»"""
    request_id: str
    start_time: float
    end_time: float
    response_time: float
    generation_time: float
    queue_time: float = 0.0
    processing_time: float = 0.0
    gpu_utilization: float = 0.0
    vram_usage: float = 0.0
    cpu_usage: float = 0.0
    memory_usage: float = 0.0
    success: bool = True
    error_message: str = ""
    optimization_type: str = "baseline"


class AdvancedFluxTester:
    """é«˜çº§ FLUX API ååé‡æµ‹è¯•å™¨"""
    
    def __init__(self, base_url: str = "http://localhost:8000"):
        self.base_url = base_url
        self.results: List[AdvancedTestResult] = []
        self.lock = threading.Lock()
        
        # æµ‹è¯•ç”¨çš„æç¤ºè¯
        self.test_prompts = [
            "ä¸€åªå¯çˆ±çš„å°çŒ«åœ¨èŠ±å›­é‡Œç©è€",
            "ç¾ä¸½çš„æ—¥è½é£æ™¯ç”»",
            "æœªæ¥ç§‘æŠ€åŸå¸‚å¤œæ™¯",
            "ä¼ ç»Ÿä¸­å›½å±±æ°´ç”»",
            "ç°ä»£æŠ½è±¡è‰ºæœ¯é£æ ¼",
            "ç«¥è¯æ•…äº‹ä¸­çš„åŸå ¡",
            "æµ·æ´‹æ·±å¤„çš„ç¥ç§˜ç”Ÿç‰©",
            "å¤ªç©ºä¸­çš„æ˜Ÿé™…é£èˆ¹",
            "æ£®æ—ä¸­çš„ç²¾çµä¸–ç•Œ",
            "å¤å¤é£æ ¼çš„è’¸æ±½æœ‹å…‹æœºæ¢°"
        ]
        
        # ç³»ç»Ÿç›‘æ§
        self.system_metrics = []
        self.monitoring_task = None
    
    async def start_monitoring(self):
        """å¯åŠ¨ç³»ç»Ÿç›‘æ§"""
        self.monitoring_task = asyncio.create_task(self._monitor_system())
    
    async def stop_monitoring(self):
        """åœæ­¢ç³»ç»Ÿç›‘æ§"""
        if self.monitoring_task:
            self.monitoring_task.cancel()
            try:
                await self.monitoring_task
            except asyncio.CancelledError:
                pass
    
    async def _monitor_system(self):
        """ç³»ç»Ÿç›‘æ§å¾ªç¯"""
        while True:
            try:
                metrics = self.get_system_metrics()
                metrics['timestamp'] = time.time()
                self.system_metrics.append(metrics)
                
                # åªä¿ç•™æœ€è¿‘1000ä¸ªæ•°æ®ç‚¹
                if len(self.system_metrics) > 1000:
                    self.system_metrics = self.system_metrics[-1000:]
                
                await asyncio.sleep(1)  # æ¯ç§’ç›‘æ§ä¸€æ¬¡
            except Exception as e:
                print(f"ç›‘æ§é”™è¯¯: {e}")
                await asyncio.sleep(5)
    
    def get_system_metrics(self) -> Dict[str, float]:
        """è·å–ç³»ç»Ÿæ€§èƒ½æŒ‡æ ‡"""
        try:
            # CPU ä½¿ç”¨ç‡
            cpu_percent = psutil.cpu_percent(interval=0.1)
            
            # å†…å­˜ä½¿ç”¨ç‡
            memory = psutil.virtual_memory()
            memory_percent = memory.percent
            
            # GPU ä¿¡æ¯
            gpu_utilization = 0.0
            vram_usage = 0.0
            
            try:
                result = subprocess.run(
                    ['nvidia-smi', '--query-gpu=utilization.gpu,memory.used,memory.total', 
                     '--format=csv,noheader,nounits'],
                    capture_output=True, text=True, timeout=5
                )
                if result.returncode == 0:
                    lines = result.stdout.strip().split('\n')
                    if lines:
                        gpu_info = lines[0].split(', ')
                        if len(gpu_info) >= 3:
                            gpu_utilization = float(gpu_info[0])
                            vram_used = float(gpu_info[1])
                            vram_total = float(gpu_info[2])
                            vram_usage = (vram_used / vram_total) * 100 if vram_total > 0 else 0
                else:
                    print(f"âš ï¸ nvidia-smi è¿”å›é”™è¯¯: {result.stderr}")
            except Exception as e:
                print(f"âš ï¸ GPU ç›‘æ§å¤±è´¥: {e}")
            
            return {
                "cpu_percent": cpu_percent,
                "memory_percent": memory_percent,
                "gpu_utilization": gpu_utilization,
                "vram_usage": vram_usage
            }
            
            return {
                "cpu_percent": cpu_percent,
                "memory_percent": memory_percent,
                "gpu_utilization": gpu_utilization,
                "vram_usage": vram_usage
            }
        except Exception:
            return {
                "cpu_percent": 0.0,
                "memory_percent": 0.0,
                "gpu_utilization": 0.0,
                "vram_usage": 0.0
            }
    
    async def test_baseline_throughput(self, num_requests: int, max_concurrent: int = 3) -> Dict[str, Any]:
        """æµ‹è¯•åŸºå‡†ååé‡"""
        print(f"ğŸ“Š æµ‹è¯•åŸºå‡†ååé‡: {num_requests} è¯·æ±‚, {max_concurrent} å¹¶å‘")
        return await self._test_with_strategy("baseline", num_requests, max_concurrent)
    
    async def test_model_pool_throughput(self, num_requests: int, max_concurrent: int = 5) -> Dict[str, Any]:
        """æµ‹è¯•æ¨¡å‹æ± ä¼˜åŒ–ååé‡"""
        print(f"ğŸŠ æµ‹è¯•æ¨¡å‹æ± ä¼˜åŒ–: {num_requests} è¯·æ±‚, {max_concurrent} å¹¶å‘")
        return await self._test_with_strategy("model_pool", num_requests, max_concurrent)
    
    async def test_batch_throughput(self, num_requests: int, max_concurrent: int = 3) -> Dict[str, Any]:
        """æµ‹è¯•æ‰¹é‡æ¨ç†ååé‡"""
        print(f"ğŸ“¦ æµ‹è¯•æ‰¹é‡æ¨ç†: {num_requests} è¯·æ±‚, {max_concurrent} å¹¶å‘")
        return await self._test_with_strategy("batch", num_requests, max_concurrent)
    
    async def test_async_throughput(self, num_requests: int, max_concurrent: int = 3) -> Dict[str, Any]:
        """æµ‹è¯•å¼‚æ­¥æ¨ç†ååé‡"""
        print(f"âš¡ æµ‹è¯•å¼‚æ­¥æ¨ç†: {num_requests} è¯·æ±‚, {max_concurrent} å¹¶å‘")
        return await self._test_with_strategy("async", num_requests, max_concurrent)
    
    async def _test_with_strategy(self, strategy: str, num_requests: int, max_concurrent: int) -> Dict[str, Any]:
        """ä½¿ç”¨æŒ‡å®šç­–ç•¥è¿›è¡Œæµ‹è¯•"""
        # å‡†å¤‡è¯·æ±‚æ•°æ®
        requests_data = []
        for i in range(num_requests):
            prompt = self.test_prompts[i % len(self.test_prompts)]
            requests_data.append((prompt, f"{strategy}_{i:04d}"))
        
        # åˆ›å»ºä¿¡å·é‡é™åˆ¶å¹¶å‘æ•°
        semaphore = asyncio.Semaphore(max_concurrent)
        
        async def limited_request(session, prompt, request_id):
            async with semaphore:
                return await self._test_single_request(session, prompt, request_id, strategy)
        
        # æ‰§è¡Œæµ‹è¯•
        start_time = time.time()
        async with aiohttp.ClientSession() as session:
            tasks = [
                limited_request(session, prompt, request_id)
                for prompt, request_id in requests_data
            ]
            results = await asyncio.gather(*tasks, return_exceptions=True)
        
        end_time = time.time()
        total_time = end_time - start_time
        
        # å¤„ç†ç»“æœ
        with self.lock:
            self.results.extend([r for r in results if isinstance(r, AdvancedTestResult)])
        
        return self.analyze_advanced_results(total_time, num_requests, strategy)
    
    async def _test_single_request(self, session: aiohttp.ClientSession, prompt: str, 
                                 request_id: str, strategy: str) -> AdvancedTestResult:
        """æµ‹è¯•å•ä¸ªè¯·æ±‚"""
        start_time = time.time()
        
        # æ ¹æ®ç­–ç•¥é€‰æ‹©ä¸åŒçš„ç«¯ç‚¹
        if strategy == "baseline":
            endpoint = "/generate"
        elif strategy == "model_pool":
            endpoint = "/generate"  # å‡è®¾æ¨¡å‹æ± ä¼˜åŒ–åœ¨åŒä¸€ä¸ªç«¯ç‚¹
        elif strategy == "batch":
            endpoint = "/generate"  # æš‚æ—¶ä½¿ç”¨ç›¸åŒç«¯ç‚¹ï¼Œå®é™…åº”è¯¥å®ç°æ‰¹é‡æ¨ç†
        elif strategy == "async":
            endpoint = "/generate"  # æš‚æ—¶ä½¿ç”¨ç›¸åŒç«¯ç‚¹ï¼Œå®é™…åº”è¯¥å®ç°å¼‚æ­¥æ¨ç†
        else:
            endpoint = "/generate"
        
        payload = {
            "prompt": prompt,
            "num_inference_steps": 25,
            "guidance_scale": 3.5,
            "width": 512,
            "height": 512
        }
        
        try:
            async with session.post(
                f"{self.base_url}{endpoint}",
                json=payload,
                timeout=aiohttp.ClientTimeout(total=300)
            ) as response:
                end_time = time.time()
                response_time = end_time - start_time
                
                if response.status == 200:
                    result_data = await response.json()
                    generation_time = float(result_data.get("generation_time", "0").replace("s", ""))
                    
                    # è·å–ç³»ç»ŸæŒ‡æ ‡
                    metrics = self.get_system_metrics()
                    
                    return AdvancedTestResult(
                        request_id=request_id,
                        start_time=start_time,
                        end_time=end_time,
                        response_time=response_time,
                        generation_time=generation_time,
                        gpu_utilization=metrics["gpu_utilization"],
                        vram_usage=metrics["vram_usage"],
                        cpu_usage=metrics["cpu_percent"],
                        memory_usage=metrics["memory_percent"],
                        success=True,
                        optimization_type=strategy
                    )
                else:
                    error_text = await response.text()
                    return AdvancedTestResult(
                        request_id=request_id,
                        start_time=start_time,
                        end_time=end_time,
                        response_time=response_time,
                        generation_time=0.0,
                        success=False,
                        error_message=error_text,
                        optimization_type=strategy
                    )
                    
        except Exception as e:
            end_time = time.time()
            return AdvancedTestResult(
                request_id=request_id,
                start_time=start_time,
                end_time=end_time,
                response_time=end_time - start_time,
                generation_time=0.0,
                success=False,
                error_message=str(e),
                optimization_type=strategy
            )
    
    def analyze_advanced_results(self, total_time: float, num_requests: int, strategy: str) -> Dict[str, Any]:
        """åˆ†æé«˜çº§æµ‹è¯•ç»“æœ"""
        strategy_results = [r for r in self.results if r.optimization_type == strategy]
        successful_results = [r for r in strategy_results if r.success]
        failed_results = [r for r in strategy_results if not r.success]
        
        if not successful_results:
            return {
                "strategy": strategy,
                "total_requests": num_requests,
                "successful_requests": 0,
                "failed_requests": len(failed_results),
                "success_rate": 0.0,
                "total_time": total_time,
                "throughput": 0.0,
                "avg_response_time": 0.0,
                "median_response_time": 0.0,
                "min_response_time": 0.0,
                "max_response_time": 0.0,
                "p50_response_time": 0.0,
                "p95_response_time": 0.0,
                "p99_response_time": 0.0,
                "response_time_std": 0.0,
                "avg_generation_time": 0.0,
                "median_generation_time": 0.0,
                "avg_gpu_utilization": 0.0,
                "avg_vram_usage": 0.0,
                "avg_cpu_usage": 0.0,
                "avg_memory_usage": 0.0,
                "efficiency_score": 0.0,
                "error": "æ²¡æœ‰æˆåŠŸçš„è¯·æ±‚"
            }
        
        # æ—¶é—´åˆ†æ
        response_times = [r.response_time for r in successful_results]
        generation_times = [r.generation_time for r in successful_results]
        
        # èµ„æºä½¿ç”¨åˆ†æ
        gpu_utilizations = [r.gpu_utilization for r in successful_results]
        vram_usages = [r.vram_usage for r in successful_results]
        cpu_usages = [r.cpu_usage for r in successful_results]
        memory_usages = [r.memory_usage for r in successful_results]
        
        # æ€§èƒ½æŒ‡æ ‡
        p50_response_time = np.percentile(response_times, 50)
        p95_response_time = np.percentile(response_times, 95)
        p99_response_time = np.percentile(response_times, 99)
        
        analysis = {
            "strategy": strategy,
            "total_requests": num_requests,
            "successful_requests": len(successful_results),
            "failed_requests": len(failed_results),
            "success_rate": len(successful_results) / num_requests * 100,
            "total_time": total_time,
            "throughput": len(successful_results) / total_time,
            
            # æ—¶é—´ç»Ÿè®¡
            "avg_response_time": statistics.mean(response_times),
            "median_response_time": statistics.median(response_times),
            "min_response_time": min(response_times),
            "max_response_time": max(response_times),
            "p50_response_time": p50_response_time,
            "p95_response_time": p95_response_time,
            "p99_response_time": p99_response_time,
            "response_time_std": statistics.stdev(response_times) if len(response_times) > 1 else 0,
            
            "avg_generation_time": statistics.mean(generation_times),
            "median_generation_time": statistics.median(generation_times),
            
            # èµ„æºä½¿ç”¨
            "avg_gpu_utilization": statistics.mean(gpu_utilizations),
            "avg_vram_usage": statistics.mean(vram_usages),
            "avg_cpu_usage": statistics.mean(cpu_usages),
            "avg_memory_usage": statistics.mean(memory_usages),
            
            # æ€§èƒ½è¯„ä¼°
            "efficiency_score": self._calculate_efficiency_score(
                len(successful_results) / total_time,
                statistics.mean(response_times),
                statistics.mean(gpu_utilizations)
            )
        }
        
        return analysis
    
    def _calculate_efficiency_score(self, throughput: float, avg_response_time: float, gpu_utilization: float) -> float:
        """è®¡ç®—æ•ˆç‡åˆ†æ•°"""
        # ç»¼åˆè€ƒè™‘ååé‡ã€å“åº”æ—¶é—´å’ŒGPUåˆ©ç”¨ç‡
        throughput_score = min(throughput / 1.0, 1.0)  # æ ‡å‡†åŒ–åˆ°1.0
        response_time_score = max(0, 1 - avg_response_time / 30.0)  # 30ç§’ä¸ºåŸºå‡†
        gpu_score = gpu_utilization / 100.0
        
        # åŠ æƒå¹³å‡
        efficiency = (throughput_score * 0.5 + response_time_score * 0.3 + gpu_score * 0.2)
        return efficiency
    
    def generate_comparison_report(self, results: List[Dict[str, Any]]):
        """ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š"""
        print(f"\n{'='*100}")
        print(f"ğŸ“Š ä¼˜åŒ–ç­–ç•¥å¯¹æ¯”æŠ¥å‘Š")
        print(f"{'='*100}")
        
        # åˆ›å»ºå¯¹æ¯”è¡¨æ ¼
        print(f"{'ç­–ç•¥':<15} {'ååé‡':<10} {'æˆåŠŸç‡':<8} {'å¹³å‡å“åº”æ—¶é—´':<12} {'P95å“åº”æ—¶é—´':<12} {'GPUåˆ©ç”¨ç‡':<10} {'æ•ˆç‡åˆ†æ•°':<10}")
        print(f"{'-'*100}")
        
        for result in results:
            strategy = result['strategy']
            throughput = result['throughput']
            success_rate = result['success_rate']
            avg_response = result['avg_response_time']
            p95_response = result['p95_response_time']
            gpu_util = result['avg_gpu_utilization']
            efficiency = result['efficiency_score']
            
            print(f"{strategy:<15} {throughput:<10.2f} {success_rate:<8.1f}% {avg_response:<12.2f}s {p95_response:<12.2f}s {gpu_util:<10.1f}% {efficiency:<10.3f}")
        
        print(f"{'='*100}")
        
        # æ‰¾å‡ºæœ€ä½³ç­–ç•¥
        best_strategy = max(results, key=lambda x: x['efficiency_score'])
        print(f"\nğŸ† æœ€ä½³ç­–ç•¥: {best_strategy['strategy']}")
        print(f"   æ•ˆç‡åˆ†æ•°: {best_strategy['efficiency_score']:.3f}")
        print(f"   ååé‡: {best_strategy['throughput']:.2f} è¯·æ±‚/ç§’")
        print(f"   å¹³å‡å“åº”æ—¶é—´: {best_strategy['avg_response_time']:.2f}ç§’")
        
        # æ€§èƒ½æå‡åˆ†æ
        baseline = next((r for r in results if r['strategy'] == 'baseline'), None)
        if baseline:
            print(f"\nğŸ“ˆ æ€§èƒ½æå‡åˆ†æ (ç›¸å¯¹äºåŸºå‡†):")
            for result in results:
                if result['strategy'] != 'baseline':
                    throughput_improvement = (result['throughput'] / baseline['throughput'] - 1) * 100
                    response_improvement = (1 - result['avg_response_time'] / baseline['avg_response_time']) * 100
                    print(f"   {result['strategy']}: ååé‡ +{throughput_improvement:.1f}%, å“åº”æ—¶é—´ -{response_improvement:.1f}%")
        
        print(f"{'='*100}\n")
    
    def plot_performance_comparison(self, results: List[Dict[str, Any]], save_path: str = "performance_comparison.png"):
        """ç»˜åˆ¶æ€§èƒ½å¯¹æ¯”å›¾è¡¨"""
        try:
            import matplotlib.pyplot as plt
            
            strategies = [r['strategy'] for r in results]
            throughputs = [r['throughput'] for r in results]
            response_times = [r['avg_response_time'] for r in results]
            efficiencies = [r['efficiency_score'] for r in results]
            
            fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))
            
            # ååé‡å¯¹æ¯”
            bars1 = ax1.bar(strategies, throughputs, color=['skyblue', 'lightgreen', 'orange', 'pink'])
            ax1.set_title('ååé‡å¯¹æ¯” (è¯·æ±‚/ç§’)')
            ax1.set_ylabel('ååé‡')
            ax1.tick_params(axis='x', rotation=45)
            
            # åœ¨æŸ±çŠ¶å›¾ä¸Šæ·»åŠ æ•°å€¼
            for bar, value in zip(bars1, throughputs):
                ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                        f'{value:.2f}', ha='center', va='bottom')
            
            # å“åº”æ—¶é—´å¯¹æ¯”
            bars2 = ax2.bar(strategies, response_times, color=['lightcoral', 'lightblue', 'lightyellow', 'lightgray'])
            ax2.set_title('å¹³å‡å“åº”æ—¶é—´å¯¹æ¯” (ç§’)')
            ax2.set_ylabel('å“åº”æ—¶é—´')
            ax2.tick_params(axis='x', rotation=45)
            
            for bar, value in zip(bars2, response_times):
                ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1,
                        f'{value:.2f}', ha='center', va='bottom')
            
            # æ•ˆç‡åˆ†æ•°å¯¹æ¯”
            bars3 = ax3.bar(strategies, efficiencies, color=['gold', 'silver', 'bronze', 'lightgreen'])
            ax3.set_title('æ•ˆç‡åˆ†æ•°å¯¹æ¯”')
            ax3.set_ylabel('æ•ˆç‡åˆ†æ•°')
            ax3.tick_params(axis='x', rotation=45)
            
            for bar, value in zip(bars3, efficiencies):
                ax3.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                        f'{value:.3f}', ha='center', va='bottom')
            
            # ç»¼åˆæ€§èƒ½é›·è¾¾å›¾
            ax4.set_title('ç»¼åˆæ€§èƒ½é›·è¾¾å›¾')
            ax4.set_aspect('equal')
            
            # æ ‡å‡†åŒ–æ•°æ®åˆ°0-1èŒƒå›´
            max_throughput = max(throughputs)
            max_response = max(response_times)
            max_efficiency = max(efficiencies)
            
            normalized_throughputs = [t/max_throughput for t in throughputs]
            normalized_response_times = [1 - rt/max_response for rt in response_times]  # åè½¬ï¼Œè¶Šå°è¶Šå¥½
            normalized_efficiencies = [e/max_efficiency for e in efficiencies]
            
            angles = np.linspace(0, 2 * np.pi, 3, endpoint=False).tolist()
            angles += angles[:1]  # é—­åˆ
            
            for i, strategy in enumerate(strategies):
                values = [normalized_throughputs[i], normalized_response_times[i], normalized_efficiencies[i]]
                values += values[:1]  # é—­åˆ
                ax4.plot(angles, values, 'o-', linewidth=2, label=strategy)
                ax4.fill(angles, values, alpha=0.25)
            
            ax4.set_xticks(angles[:-1])
            ax4.set_xticklabels(['ååé‡', 'å“åº”æ—¶é—´', 'æ•ˆç‡åˆ†æ•°'])
            ax4.legend()
            
            plt.tight_layout()
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"ğŸ“Š æ€§èƒ½å¯¹æ¯”å›¾è¡¨å·²ä¿å­˜åˆ°: {save_path}")
            plt.show()
            
        except ImportError:
            print("âŒ matplotlib æœªå®‰è£…ï¼Œè·³è¿‡å›¾è¡¨ç”Ÿæˆ")
        except Exception as e:
            print(f"âŒ å›¾è¡¨ç”Ÿæˆå¤±è´¥: {e}")


async def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="FLUX API é«˜çº§ååé‡æµ‹è¯•å·¥å…·")
    parser.add_argument("--url", default="http://localhost:8000", help="API åŸºç¡€URL")
    parser.add_argument("--requests", type=int, default=20, help="æ¯ä¸ªç­–ç•¥çš„æµ‹è¯•è¯·æ±‚æ•°é‡")
    parser.add_argument("--concurrent", type=int, default=3, help="æœ€å¤§å¹¶å‘æ•°")
    parser.add_argument("--strategies", nargs='+', 
                       choices=['baseline', 'model_pool', 'batch', 'async', 'all'],
                       default=['baseline'], help="æµ‹è¯•ç­–ç•¥")
    parser.add_argument("--plot", action="store_true", help="ç”Ÿæˆå¯¹æ¯”å›¾è¡¨")
    parser.add_argument("--save", action="store_true", help="ä¿å­˜è¯¦ç»†ç»“æœ")
    
    args = parser.parse_args()
    
    print("ğŸš€ FLUX API é«˜çº§ååé‡æµ‹è¯•å·¥å…·")
    print(f"ğŸ“ API URL: {args.url}")
    print(f"ğŸ“Š æµ‹è¯•é…ç½®: {args.requests} è¯·æ±‚/ç­–ç•¥, {args.concurrent} å¹¶å‘")
    print(f"ğŸ¯ æµ‹è¯•ç­–ç•¥: {args.strategies}")
    
    tester = AdvancedFluxTester(args.url)
    
    # å¯åŠ¨ç³»ç»Ÿç›‘æ§
    await tester.start_monitoring()
    
    try:
        # ç¡®å®šè¦æµ‹è¯•çš„ç­–ç•¥
        if 'all' in args.strategies:
            strategies_to_test = ['baseline', 'model_pool', 'batch', 'async']
        else:
            strategies_to_test = args.strategies
        
        print(f"ğŸ¯ å®é™…æµ‹è¯•ç­–ç•¥: {strategies_to_test}")
        
        results = []
        
        # æ‰§è¡Œæµ‹è¯•
        for strategy in strategies_to_test:
            if strategy == 'baseline':
                result = await tester.test_baseline_throughput(args.requests, args.concurrent)
            elif strategy == 'model_pool':
                result = await tester.test_model_pool_throughput(args.requests, args.concurrent)
            elif strategy == 'batch':
                result = await tester.test_batch_throughput(args.requests, args.concurrent)
            elif strategy == 'async':
                result = await tester.test_async_throughput(args.requests, args.concurrent)
            
            results.append(result)
            
            # ç­‰å¾…ä¸€æ®µæ—¶é—´å†è¿›è¡Œä¸‹ä¸€ä¸ªæµ‹è¯•
            if len(strategies_to_test) > 1:
                print(f"â³ ç­‰å¾… 10 ç§’åè¿›è¡Œä¸‹ä¸€ä¸ªæµ‹è¯•...")
                await asyncio.sleep(10)
        
        # ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š
        tester.generate_comparison_report(results)
        
        # ç”Ÿæˆå›¾è¡¨
        if args.plot:
            tester.plot_performance_comparison(results)
        
        # ä¿å­˜è¯¦ç»†ç»“æœ
        if args.save:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            results_file = f"advanced_throughput_test_{timestamp}.json"
            
            detailed_results = {
                "test_config": {
                    "url": args.url,
                    "requests_per_strategy": args.requests,
                    "concurrent": args.concurrent,
                    "strategies": strategies_to_test
                },
                "performance_results": results,
                "system_metrics": tester.system_metrics[-100:],  # æœ€å100ä¸ªæ•°æ®ç‚¹
                "detailed_results": [
                    {
                        "request_id": r.request_id,
                        "optimization_type": r.optimization_type,
                        "response_time": r.response_time,
                        "generation_time": r.generation_time,
                        "gpu_utilization": r.gpu_utilization,
                        "vram_usage": r.vram_usage,
                        "cpu_usage": r.cpu_usage,
                        "memory_usage": r.memory_usage,
                        "success": r.success,
                        "error_message": r.error_message
                    }
                    for r in tester.results
                ]
            }
            
            with open(results_file, 'w', encoding='utf-8') as f:
                json.dump(detailed_results, f, indent=2, ensure_ascii=False)
            
            print(f"ğŸ“„ è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: {results_file}")
    
    finally:
        # åœæ­¢ç›‘æ§
        await tester.stop_monitoring()


if __name__ == "__main__":
    asyncio.run(main()) 