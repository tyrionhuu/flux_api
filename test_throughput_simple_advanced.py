#!/usr/bin/env python3
"""
FLUX API ç®€åŒ–ç‰ˆé«˜çº§ååé‡æµ‹è¯•å·¥å…·
åªæµ‹è¯•å½“å‰å¯ç”¨çš„ç«¯ç‚¹ï¼Œé¿å…ç«¯ç‚¹ä¸å­˜åœ¨çš„é—®é¢˜
"""

import asyncio
import aiohttp
import time
import json
import statistics
import argparse
from typing import List, Dict, Any
from dataclasses import dataclass
from datetime import datetime
import threading
import psutil
import subprocess
import numpy as np


@dataclass
class SimpleTestResult:
    """ç®€åŒ–æµ‹è¯•ç»“æœæ•°æ®ç±»"""
    request_id: str
    start_time: float
    end_time: float
    response_time: float
    generation_time: float
    gpu_utilization: float = 0.0
    vram_usage: float = 0.0
    cpu_usage: float = 0.0
    memory_usage: float = 0.0
    success: bool = True
    error_message: str = ""
    test_type: str = "baseline"


class SimpleAdvancedTester:
    """ç®€åŒ–ç‰ˆé«˜çº§ FLUX API ååé‡æµ‹è¯•å™¨"""
    
    def __init__(self, base_url: str = "http://localhost:8000"):
        self.base_url = base_url
        self.results: List[SimpleTestResult] = []
        self.lock = threading.Lock()
        
        # æµ‹è¯•ç”¨çš„æç¤ºè¯
        self.test_prompts = [
            "ä¸€åªå¯çˆ±çš„å°çŒ«åœ¨èŠ±å›­é‡Œç©è€",
            "ç¾ä¸½çš„æ—¥è½é£æ™¯ç”»",
            "æœªæ¥ç§‘æŠ€åŸå¸‚å¤œæ™¯",
            "ä¼ ç»Ÿä¸­å›½å±±æ°´ç”»",
            "ç°ä»£æŠ½è±¡è‰ºæœ¯é£æ ¼",
            "ç«¥è¯æ•…äº‹ä¸­çš„åŸå ¡",
            "æµ·æ´‹æ·±å¤„çš„ç¥ç§˜ç”Ÿç‰©",
            "å¤ªç©ºä¸­çš„æ˜Ÿé™…é£èˆ¹",
            "æ£®æ—ä¸­çš„ç²¾çµä¸–ç•Œ",
            "å¤å¤é£æ ¼çš„è’¸æ±½æœ‹å…‹æœºæ¢°"
        ]
    
    def get_system_metrics(self) -> Dict[str, float]:
        """è·å–ç³»ç»Ÿæ€§èƒ½æŒ‡æ ‡"""
        try:
            # CPU ä½¿ç”¨ç‡
            cpu_percent = psutil.cpu_percent(interval=0.1)
            
            # å†…å­˜ä½¿ç”¨ç‡
            memory = psutil.virtual_memory()
            memory_percent = memory.percent
            
            # GPU ä¿¡æ¯
            gpu_utilization = 0.0
            vram_usage = 0.0
            
            try:
                result = subprocess.run(
                    ['nvidia-smi', '--query-gpu=utilization.gpu,memory.used,memory.total', 
                     '--format=csv,noheader,nounits'],
                    capture_output=True, text=True, timeout=5
                )
                if result.returncode == 0:
                    lines = result.stdout.strip().split('\n')
                    if lines:
                        gpu_info = lines[0].split(', ')
                        if len(gpu_info) >= 3:
                            gpu_utilization = float(gpu_info[0])
                            vram_used = float(gpu_info[1])
                            vram_total = float(gpu_info[2])
                            vram_usage = (vram_used / vram_total) * 100 if vram_total > 0 else 0
            except Exception as e:
                print(f"âš ï¸ GPU ç›‘æ§å¤±è´¥: {e}")
            
            return {
                "cpu_percent": cpu_percent,
                "memory_percent": memory_percent,
                "gpu_utilization": gpu_utilization,
                "vram_usage": vram_usage
            }
        except Exception:
            return {
                "cpu_percent": 0.0,
                "memory_percent": 0.0,
                "gpu_utilization": 0.0,
                "vram_usage": 0.0
            }
    
    async def test_baseline_throughput(self, num_requests: int, max_concurrent: int = 3) -> Dict[str, Any]:
        """æµ‹è¯•åŸºå‡†ååé‡"""
        print(f"ğŸ“Š æµ‹è¯•åŸºå‡†ååé‡: {num_requests} è¯·æ±‚, {max_concurrent} å¹¶å‘")
        return await self._test_with_type("baseline", num_requests, max_concurrent)
    
    async def test_high_concurrent_throughput(self, num_requests: int, max_concurrent: int = 5) -> Dict[str, Any]:
        """æµ‹è¯•é«˜å¹¶å‘ååé‡"""
        print(f"ğŸŠ æµ‹è¯•é«˜å¹¶å‘ååé‡: {num_requests} è¯·æ±‚, {max_concurrent} å¹¶å‘")
        return await self._test_with_type("high_concurrent", num_requests, max_concurrent)
    
    async def test_low_steps_throughput(self, num_requests: int, max_concurrent: int = 3) -> Dict[str, Any]:
        """æµ‹è¯•ä½æ¨ç†æ­¥æ•°ååé‡"""
        print(f"âš¡ æµ‹è¯•ä½æ¨ç†æ­¥æ•°: {num_requests} è¯·æ±‚, {max_concurrent} å¹¶å‘")
        return await self._test_with_type("low_steps", num_requests, max_concurrent)
    
    async def test_small_image_throughput(self, num_requests: int, max_concurrent: int = 3) -> Dict[str, Any]:
        """æµ‹è¯•å°å›¾åƒå°ºå¯¸ååé‡"""
        print(f"ğŸ–¼ï¸ æµ‹è¯•å°å›¾åƒå°ºå¯¸: {num_requests} è¯·æ±‚, {max_concurrent} å¹¶å‘")
        return await self._test_with_type("small_image", num_requests, max_concurrent)
    
    async def _test_with_type(self, test_type: str, num_requests: int, max_concurrent: int) -> Dict[str, Any]:
        """ä½¿ç”¨æŒ‡å®šç±»å‹è¿›è¡Œæµ‹è¯•"""
        # å‡†å¤‡è¯·æ±‚æ•°æ®
        requests_data = []
        for i in range(num_requests):
            prompt = self.test_prompts[i % len(self.test_prompts)]
            requests_data.append((prompt, f"{test_type}_{i:04d}"))
        
        # åˆ›å»ºä¿¡å·é‡é™åˆ¶å¹¶å‘æ•°
        semaphore = asyncio.Semaphore(max_concurrent)
        
        async def limited_request(session, prompt, request_id):
            async with semaphore:
                return await self._test_single_request(session, prompt, request_id, test_type)
        
        # æ‰§è¡Œæµ‹è¯•
        start_time = time.time()
        async with aiohttp.ClientSession() as session:
            tasks = [
                limited_request(session, prompt, request_id)
                for prompt, request_id in requests_data
            ]
            results = await asyncio.gather(*tasks, return_exceptions=True)
        
        end_time = time.time()
        total_time = end_time - start_time
        
        # å¤„ç†ç»“æœ
        with self.lock:
            self.results.extend([r for r in results if isinstance(r, SimpleTestResult)])
        
        return self.analyze_simple_results(total_time, num_requests, test_type)
    
    async def _test_single_request(self, session: aiohttp.ClientSession, prompt: str, 
                                 request_id: str, test_type: str) -> SimpleTestResult:
        """æµ‹è¯•å•ä¸ªè¯·æ±‚"""
        start_time = time.time()
        
        # æ ¹æ®æµ‹è¯•ç±»å‹å‡†å¤‡ä¸åŒçš„å‚æ•°
        if test_type == "baseline":
            payload = {
                "prompt": prompt,
                "num_inference_steps": 25,
                "guidance_scale": 3.5,
                "width": 512,
                "height": 512
            }
        elif test_type == "high_concurrent":
            payload = {
                "prompt": prompt,
                "num_inference_steps": 25,
                "guidance_scale": 3.5,
                "width": 512,
                "height": 512
            }
        elif test_type == "low_steps":
            payload = {
                "prompt": prompt,
                "num_inference_steps": 15,  # å‡å°‘æ¨ç†æ­¥æ•°
                "guidance_scale": 3.5,
                "width": 512,
                "height": 512
            }
        elif test_type == "small_image":
            payload = {
                "prompt": prompt,
                "num_inference_steps": 25,
                "guidance_scale": 3.5,
                "width": 384,  # å‡å°å›¾åƒå°ºå¯¸
                "height": 384
            }
        else:
            payload = {
                "prompt": prompt,
                "num_inference_steps": 25,
                "guidance_scale": 3.5,
                "width": 512,
                "height": 512
            }
        
        try:
            async with session.post(
                f"{self.base_url}/generate",
                json=payload,
                timeout=aiohttp.ClientTimeout(total=300)
            ) as response:
                end_time = time.time()
                response_time = end_time - start_time
                
                if response.status == 200:
                    result_data = await response.json()
                    generation_time = float(result_data.get("generation_time", "0").replace("s", ""))
                    
                    # è·å–ç³»ç»ŸæŒ‡æ ‡
                    metrics = self.get_system_metrics()
                    
                    return SimpleTestResult(
                        request_id=request_id,
                        start_time=start_time,
                        end_time=end_time,
                        response_time=response_time,
                        generation_time=generation_time,
                        gpu_utilization=metrics["gpu_utilization"],
                        vram_usage=metrics["vram_usage"],
                        cpu_usage=metrics["cpu_percent"],
                        memory_usage=metrics["memory_percent"],
                        success=True,
                        test_type=test_type
                    )
                else:
                    error_text = await response.text()
                    return SimpleTestResult(
                        request_id=request_id,
                        start_time=start_time,
                        end_time=end_time,
                        response_time=response_time,
                        generation_time=0.0,
                        success=False,
                        error_message=error_text,
                        test_type=test_type
                    )
                    
        except Exception as e:
            end_time = time.time()
            return SimpleTestResult(
                request_id=request_id,
                start_time=start_time,
                end_time=end_time,
                response_time=end_time - start_time,
                generation_time=0.0,
                success=False,
                error_message=str(e),
                test_type=test_type
            )
    
    def analyze_simple_results(self, total_time: float, num_requests: int, test_type: str) -> Dict[str, Any]:
        """åˆ†æç®€åŒ–æµ‹è¯•ç»“æœ"""
        type_results = [r for r in self.results if r.test_type == test_type]
        successful_results = [r for r in type_results if r.success]
        failed_results = [r for r in type_results if not r.success]
        
        if not successful_results:
            return {
                "test_type": test_type,
                "total_requests": num_requests,
                "successful_requests": 0,
                "failed_requests": len(failed_results),
                "success_rate": 0.0,
                "total_time": total_time,
                "throughput": 0.0,
                "avg_response_time": 0.0,
                "median_response_time": 0.0,
                "min_response_time": 0.0,
                "max_response_time": 0.0,
                "p95_response_time": 0.0,
                "avg_generation_time": 0.0,
                "avg_gpu_utilization": 0.0,
                "avg_vram_usage": 0.0,
                "avg_cpu_usage": 0.0,
                "avg_memory_usage": 0.0,
                "efficiency_score": 0.0,
                "error": "æ²¡æœ‰æˆåŠŸçš„è¯·æ±‚"
            }
        
        # æ—¶é—´åˆ†æ
        response_times = [r.response_time for r in successful_results]
        generation_times = [r.generation_time for r in successful_results]
        
        # èµ„æºä½¿ç”¨åˆ†æ
        gpu_utilizations = [r.gpu_utilization for r in successful_results]
        vram_usages = [r.vram_usage for r in successful_results]
        cpu_usages = [r.cpu_usage for r in successful_results]
        memory_usages = [r.memory_usage for r in successful_results]
        
        # æ€§èƒ½æŒ‡æ ‡
        p95_response_time = np.percentile(response_times, 95)
        
        analysis = {
            "test_type": test_type,
            "total_requests": num_requests,
            "successful_requests": len(successful_results),
            "failed_requests": len(failed_results),
            "success_rate": len(successful_results) / num_requests * 100,
            "total_time": total_time,
            "throughput": len(successful_results) / total_time,
            
            # æ—¶é—´ç»Ÿè®¡
            "avg_response_time": statistics.mean(response_times),
            "median_response_time": statistics.median(response_times),
            "min_response_time": min(response_times),
            "max_response_time": max(response_times),
            "p95_response_time": p95_response_time,
            "response_time_std": statistics.stdev(response_times) if len(response_times) > 1 else 0,
            
            "avg_generation_time": statistics.mean(generation_times),
            "median_generation_time": statistics.median(generation_times),
            
            # èµ„æºä½¿ç”¨
            "avg_gpu_utilization": statistics.mean(gpu_utilizations),
            "avg_vram_usage": statistics.mean(vram_usages),
            "avg_cpu_usage": statistics.mean(cpu_usages),
            "avg_memory_usage": statistics.mean(memory_usages),
            
            # æ€§èƒ½è¯„ä¼°
            "efficiency_score": self._calculate_efficiency_score(
                len(successful_results) / total_time,
                statistics.mean(response_times),
                statistics.mean(gpu_utilizations)
            )
        }
        
        return analysis
    
    def _calculate_efficiency_score(self, throughput: float, avg_response_time: float, gpu_utilization: float) -> float:
        """è®¡ç®—æ•ˆç‡åˆ†æ•°"""
        # ç»¼åˆè€ƒè™‘ååé‡ã€å“åº”æ—¶é—´å’ŒGPUåˆ©ç”¨ç‡
        throughput_score = min(throughput / 1.0, 1.0)  # æ ‡å‡†åŒ–åˆ°1.0
        response_time_score = max(0, 1 - avg_response_time / 30.0)  # 30ç§’ä¸ºåŸºå‡†
        gpu_score = gpu_utilization / 100.0
        
        # åŠ æƒå¹³å‡
        efficiency = (throughput_score * 0.5 + response_time_score * 0.3 + gpu_score * 0.2)
        return efficiency
    
    def generate_comparison_report(self, results: List[Dict[str, Any]]):
        """ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š"""
        print(f"\n{'='*100}")
        print(f"ğŸ“Š æµ‹è¯•ç±»å‹å¯¹æ¯”æŠ¥å‘Š")
        print(f"{'='*100}")
        
        # åˆ›å»ºå¯¹æ¯”è¡¨æ ¼
        print(f"{'æµ‹è¯•ç±»å‹':<15} {'ååé‡':<10} {'æˆåŠŸç‡':<8} {'å¹³å‡å“åº”æ—¶é—´':<12} {'P95å“åº”æ—¶é—´':<12} {'GPUåˆ©ç”¨ç‡':<10} {'æ•ˆç‡åˆ†æ•°':<10}")
        print(f"{'-'*100}")
        
        for result in results:
            test_type = result['test_type']
            throughput = result['throughput']
            success_rate = result['success_rate']
            avg_response = result['avg_response_time']
            p95_response = result['p95_response_time']
            gpu_util = result['avg_gpu_utilization']
            efficiency = result['efficiency_score']
            
            print(f"{test_type:<15} {throughput:<10.2f} {success_rate:<8.1f}% {avg_response:<12.2f}s {p95_response:<12.2f}s {gpu_util:<10.1f}% {efficiency:<10.3f}")
        
        print(f"{'='*100}")
        
        # æ‰¾å‡ºæœ€ä½³æµ‹è¯•ç±»å‹
        best_type = max(results, key=lambda x: x['efficiency_score'])
        print(f"\nğŸ† æœ€ä½³æµ‹è¯•ç±»å‹: {best_type['test_type']}")
        print(f"   æ•ˆç‡åˆ†æ•°: {best_type['efficiency_score']:.3f}")
        print(f"   ååé‡: {best_type['throughput']:.2f} è¯·æ±‚/ç§’")
        print(f"   å¹³å‡å“åº”æ—¶é—´: {best_type['avg_response_time']:.2f}ç§’")
        
        # æ€§èƒ½æå‡åˆ†æ
        baseline = next((r for r in results if r['test_type'] == 'baseline'), None)
        if baseline:
            print(f"\nğŸ“ˆ æ€§èƒ½æå‡åˆ†æ (ç›¸å¯¹äºåŸºå‡†):")
            for result in results:
                if result['test_type'] != 'baseline':
                    throughput_improvement = (result['throughput'] / baseline['throughput'] - 1) * 100
                    response_improvement = (1 - result['avg_response_time'] / baseline['avg_response_time']) * 100
                    print(f"   {result['test_type']}: ååé‡ +{throughput_improvement:.1f}%, å“åº”æ—¶é—´ -{response_improvement:.1f}%")
        
        print(f"{'='*100}\n")


async def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="FLUX API ç®€åŒ–ç‰ˆé«˜çº§ååé‡æµ‹è¯•å·¥å…·")
    parser.add_argument("--url", default="http://localhost:8000", help="API åŸºç¡€URL")
    parser.add_argument("--requests", type=int, default=20, help="æ¯ä¸ªæµ‹è¯•ç±»å‹çš„è¯·æ±‚æ•°é‡")
    parser.add_argument("--concurrent", type=int, default=3, help="æœ€å¤§å¹¶å‘æ•°")
    parser.add_argument("--test-types", nargs='+', 
                       choices=['baseline', 'high_concurrent', 'low_steps', 'small_image', 'all'],
                       default=['baseline'], help="æµ‹è¯•ç±»å‹")
    parser.add_argument("--save", action="store_true", help="ä¿å­˜è¯¦ç»†ç»“æœ")
    
    args = parser.parse_args()
    
    print("ğŸš€ FLUX API ç®€åŒ–ç‰ˆé«˜çº§ååé‡æµ‹è¯•å·¥å…·")
    print(f"ğŸ“ API URL: {args.url}")
    print(f"ğŸ“Š æµ‹è¯•é…ç½®: {args.requests} è¯·æ±‚/ç±»å‹, {args.concurrent} å¹¶å‘")
    print(f"ğŸ¯ æµ‹è¯•ç±»å‹: {args.test_types}")
    
    tester = SimpleAdvancedTester(args.url)
    
    # ç¡®å®šè¦æµ‹è¯•çš„ç±»å‹
    if 'all' in args.test_types:
        test_types_to_test = ['baseline', 'high_concurrent', 'low_steps', 'small_image']
    else:
        test_types_to_test = args.test_types
    
    print(f"ğŸ¯ å®é™…æµ‹è¯•ç±»å‹: {test_types_to_test}")
    
    results = []
    
    # æ‰§è¡Œæµ‹è¯•
    for test_type in test_types_to_test:
        if test_type == 'baseline':
            result = await tester.test_baseline_throughput(args.requests, args.concurrent)
        elif test_type == 'high_concurrent':
            result = await tester.test_high_concurrent_throughput(args.requests, args.concurrent + 2)
        elif test_type == 'low_steps':
            result = await tester.test_low_steps_throughput(args.requests, args.concurrent)
        elif test_type == 'small_image':
            result = await tester.test_small_image_throughput(args.requests, args.concurrent)
        
        results.append(result)
        
        # ç­‰å¾…ä¸€æ®µæ—¶é—´å†è¿›è¡Œä¸‹ä¸€ä¸ªæµ‹è¯•
        if len(test_types_to_test) > 1:
            print(f"â³ ç­‰å¾… 10 ç§’åè¿›è¡Œä¸‹ä¸€ä¸ªæµ‹è¯•...")
            await asyncio.sleep(10)
    
    # ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š
    tester.generate_comparison_report(results)
    
    # ä¿å­˜è¯¦ç»†ç»“æœ
    if args.save:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        results_file = f"simple_advanced_test_{timestamp}.json"
        
        detailed_results = {
            "test_config": {
                "url": args.url,
                "requests_per_type": args.requests,
                "concurrent": args.concurrent,
                "test_types": test_types_to_test
            },
            "performance_results": results,
            "detailed_results": [
                {
                    "request_id": r.request_id,
                    "test_type": r.test_type,
                    "response_time": r.response_time,
                    "generation_time": r.generation_time,
                    "gpu_utilization": r.gpu_utilization,
                    "vram_usage": r.vram_usage,
                    "cpu_usage": r.cpu_usage,
                    "memory_usage": r.memory_usage,
                    "success": r.success,
                    "error_message": r.error_message
                }
                for r in tester.results
            ]
        }
        
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(detailed_results, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ“„ è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: {results_file}")


if __name__ == "__main__":
    asyncio.run(main()) 