# FLUX API ååé‡ä¼˜åŒ–åˆ†æ

## ğŸš€ ä¸»è¦æ€§èƒ½ç“¶é¢ˆ

### 1. **æ¨¡å‹å¹¶å‘è®¿é—®ç“¶é¢ˆ** â­â­â­â­â­
**é—®é¢˜**: å…¨å±€é” `_model_manager_lock` å¯¼è‡´ä¸²è¡ŒåŒ–
```python
# å½“å‰ä»£ç  - ä¸¥é‡ç“¶é¢ˆ
with _model_manager_lock:
    # æ•´ä¸ªæ¨ç†è¿‡ç¨‹éƒ½è¢«é”ä½
    result = model_manager.generate_image(...)
```

**å½±å“**: 
- å³ä½¿æœ‰å¤šä¸ª GPUï¼Œä¹Ÿåªèƒ½ä¸²è¡Œå¤„ç†è¯·æ±‚
- å¹¶å‘è¯·æ±‚ä¼šæ’é˜Ÿç­‰å¾…ï¼Œå¢åŠ å»¶è¿Ÿ
- æ— æ³•å……åˆ†åˆ©ç”¨ç¡¬ä»¶èµ„æº

**ä¼˜åŒ–æ–¹æ¡ˆ**:
```python
# ä¼˜åŒ–å - æ¨¡å‹æ± æ¨¡å¼
class ModelPool:
    def __init__(self, pool_size=3):
        self.models = [FluxModelManager() for _ in range(pool_size)]
        self.available = asyncio.Queue()
        for model in self.models:
            self.available.put_nowait(model)
    
    async def get_model(self):
        return await self.available.get()
    
    async def release_model(self, model):
        await self.available.put(model)
```

### 2. **æ¨ç†è¿‡ç¨‹ä¼˜åŒ–** â­â­â­â­
**é—®é¢˜**: æ¯æ¬¡æ¨ç†éƒ½æ˜¯ç‹¬ç«‹æ‰§è¡Œï¼Œæ²¡æœ‰æ‰¹å¤„ç†
```python
# å½“å‰æ¨ç†
result = self.pipe(**generation_kwargs)  # ä¸²è¡Œæ‰§è¡Œ
```

**ä¼˜åŒ–æ–¹æ¡ˆ**:
```python
# æ‰¹é‡æ¨ç†
@torch.compile
def batch_generate(self, prompts, **kwargs):
    # æ‰¹é‡å¤„ç†å¤šä¸ªæç¤ºè¯
    return self.pipe(prompts, **kwargs)

# CUDA Graph ä¼˜åŒ–
def enable_cuda_graph(self):
    self.pipe.enable_model_cpu_offload()
    self.pipe.enable_attention_slicing()
```

### 3. **å†…å­˜ç®¡ç†ä¼˜åŒ–** â­â­â­
**é—®é¢˜**: æ¯æ¬¡è¯·æ±‚é‡æ–°åˆ†é…å†…å­˜
```python
# å½“å‰å†…å­˜åˆ†é…
generation_kwargs = {
    "prompt": prompt,
    "num_inference_steps": num_inference_steps,
    # æ¯æ¬¡éƒ½é‡æ–°åˆ›å»º
}
```

**ä¼˜åŒ–æ–¹æ¡ˆ**:
```python
# å†…å­˜æ± 
class MemoryPool:
    def __init__(self):
        self.tensor_pool = {}
    
    def get_tensor(self, shape, dtype):
        key = (shape, dtype)
        if key in self.tensor_pool:
            return self.tensor_pool[key].pop()
        return torch.empty(shape, dtype=dtype)
    
    def return_tensor(self, tensor):
        key = (tensor.shape, tensor.dtype)
        if key not in self.tensor_pool:
            self.tensor_pool[key] = []
        self.tensor_pool[key].append(tensor)
```

## ğŸ› ï¸ å…·ä½“ä¼˜åŒ–å®ç°

### ä¼˜åŒ– 1: æ¨¡å‹æ± å®ç°
```python
# models/model_pool.py
import asyncio
import threading
from typing import List, Optional
from models.fp4_flux_model import FluxModelManager

class FluxModelPool:
    def __init__(self, pool_size: int = 3):
        self.pool_size = pool_size
        self.models: List[FluxModelManager] = []
        self.available = asyncio.Queue(maxsize=pool_size)
        self.lock = threading.Lock()
        self._initialized = False
    
    async def initialize(self):
        """åˆå§‹åŒ–æ¨¡å‹æ± """
        if self._initialized:
            return
        
        with self.lock:
            if self._initialized:
                return
            
            print(f"ğŸ”„ åˆå§‹åŒ–æ¨¡å‹æ± ï¼Œå¤§å°: {self.pool_size}")
            for i in range(self.pool_size):
                model = FluxModelManager()
                if model.load_model():
                    self.models.append(model)
                    await self.available.put(model)
                    print(f"âœ… æ¨¡å‹ {i+1} åŠ è½½æˆåŠŸ")
                else:
                    print(f"âŒ æ¨¡å‹ {i+1} åŠ è½½å¤±è´¥")
            
            self._initialized = True
    
    async def get_model(self) -> Optional[FluxModelManager]:
        """è·å–å¯ç”¨æ¨¡å‹"""
        if not self._initialized:
            await self.initialize()
        
        try:
            return await asyncio.wait_for(self.available.get(), timeout=30.0)
        except asyncio.TimeoutError:
            return None
    
    async def release_model(self, model: FluxModelManager):
        """é‡Šæ”¾æ¨¡å‹å›æ± """
        if model in self.models:
            await self.available.put(model)
    
    def get_pool_status(self) -> dict:
        """è·å–æ± çŠ¶æ€"""
        return {
            "pool_size": self.pool_size,
            "available_models": self.available.qsize(),
            "total_models": len(self.models),
            "initialized": self._initialized
        }
```

### ä¼˜åŒ– 2: æ‰¹é‡æ¨ç†å®ç°
```python
# models/batch_inference.py
import torch
from typing import List, Dict, Any

class BatchInferenceManager:
    def __init__(self, model_pool: FluxModelPool):
        self.model_pool = model_pool
        self.batch_size = 4  # å¯é…ç½®çš„æ‰¹å¤§å°
        self.pending_requests = []
        self.batch_lock = threading.Lock()
    
    async def add_request(self, prompt: str, **kwargs) -> str:
        """æ·»åŠ è¯·æ±‚åˆ°æ‰¹å¤„ç†é˜Ÿåˆ—"""
        request_id = str(uuid.uuid4())
        request = {
            "id": request_id,
            "prompt": prompt,
            "kwargs": kwargs,
            "future": asyncio.Future()
        }
        
        with self.batch_lock:
            self.pending_requests.append(request)
            
            # å¦‚æœè¾¾åˆ°æ‰¹å¤§å°ï¼Œç«‹å³å¤„ç†
            if len(self.pending_requests) >= self.batch_size:
                await self._process_batch()
        
        return await request["future"]
    
    async def _process_batch(self):
        """å¤„ç†æ‰¹é‡è¯·æ±‚"""
        if not self.pending_requests:
            return
        
        # è·å–æ¨¡å‹
        model = await self.model_pool.get_model()
        if not model:
            # å¤„ç†å¤±è´¥ï¼Œè¿”å›é”™è¯¯
            for request in self.pending_requests:
                request["future"].set_exception(Exception("No available model"))
            self.pending_requests.clear()
            return
        
        try:
            # å‡†å¤‡æ‰¹é‡æ•°æ®
            prompts = [req["prompt"] for req in self.pending_requests]
            kwargs = self.pending_requests[0]["kwargs"]  # ä½¿ç”¨ç¬¬ä¸€ä¸ªè¯·æ±‚çš„å‚æ•°
            
            # æ‰¹é‡æ¨ç†
            results = await self._batch_generate(model, prompts, **kwargs)
            
            # è®¾ç½®ç»“æœ
            for i, request in enumerate(self.pending_requests):
                if i < len(results):
                    request["future"].set_result(results[i])
                else:
                    request["future"].set_exception(Exception("Batch processing error"))
        
        finally:
            # é‡Šæ”¾æ¨¡å‹
            await self.model_pool.release_model(model)
            self.pending_requests.clear()
    
    async def _batch_generate(self, model: FluxModelManager, prompts: List[str], **kwargs):
        """æ‰§è¡Œæ‰¹é‡æ¨ç†"""
        # è¿™é‡Œéœ€è¦ä¿®æ”¹ FluxModelManager ä»¥æ”¯æŒæ‰¹é‡æ¨ç†
        # æˆ–è€…ä½¿ç”¨ torch.compile ä¼˜åŒ–å•ä¸ªæ¨ç†
        results = []
        for prompt in prompts:
            result = model.generate_image(prompt, **kwargs)
            results.append(result)
        return results
```

### ä¼˜åŒ– 3: å†…å­˜ä¼˜åŒ–
```python
# utils/memory_pool.py
import torch
from typing import Dict, List, Tuple, Any
import gc

class TensorPool:
    def __init__(self):
        self.pools: Dict[Tuple, List[torch.Tensor]] = {}
        self.max_pool_size = 10
    
    def get_tensor(self, shape: Tuple[int, ...], dtype: torch.dtype, device: str = "cuda"):
        """è·å–å¼ é‡"""
        key = (shape, dtype, device)
        
        if key in self.pools and self.pools[key]:
            tensor = self.pools[key].pop()
            tensor.zero_()  # æ¸…é›¶å¼ é‡
            return tensor
        
        return torch.empty(shape, dtype=dtype, device=device)
    
    def return_tensor(self, tensor: torch.Tensor):
        """è¿”å›å¼ é‡åˆ°æ± """
        if tensor is None:
            return
        
        key = (tensor.shape, tensor.dtype, tensor.device)
        
        if key not in self.pools:
            self.pools[key] = []
        
        if len(self.pools[key]) < self.max_pool_size:
            self.pools[key].append(tensor.detach())
    
    def clear(self):
        """æ¸…ç©ºæ± """
        for tensors in self.pools.values():
            for tensor in tensors:
                del tensor
        self.pools.clear()
        gc.collect()
        torch.cuda.empty_cache()

# å…¨å±€å¼ é‡æ± 
tensor_pool = TensorPool()
```

## ğŸ“Š æ€§èƒ½æå‡é¢„æœŸ

### å½“å‰æ€§èƒ½åŸºå‡†
- ååé‡: ~0.2-0.5 è¯·æ±‚/ç§’
- å¹³å‡å“åº”æ—¶é—´: 10-20 ç§’
- GPU åˆ©ç”¨ç‡: 30-50%

### ä¼˜åŒ–åé¢„æœŸ
- **ååé‡**: 2-5x æå‡ (0.4-2.5 è¯·æ±‚/ç§’)
- **å“åº”æ—¶é—´**: 30-50% å‡å°‘
- **GPU åˆ©ç”¨ç‡**: 70-90%

### å…·ä½“ä¼˜åŒ–æ•ˆæœ

| ä¼˜åŒ–é¡¹ç›® | é¢„æœŸæå‡ | å®ç°éš¾åº¦ |
|---------|----------|----------|
| æ¨¡å‹æ±  | 3-5x ååé‡ | ä¸­ç­‰ |
| æ‰¹é‡æ¨ç† | 1.5-2x ååé‡ | é«˜ |
| å†…å­˜ä¼˜åŒ– | 20-30% å“åº”æ—¶é—´ | ä½ |
| CUDA Graph | 10-20% å“åº”æ—¶é—´ | ä¸­ç­‰ |

## ğŸ¯ å®æ–½å»ºè®®

### é˜¶æ®µ 1: å¿«é€Ÿä¼˜åŒ– (1-2 å¤©)
1. å®ç°æ¨¡å‹æ± æ¨¡å¼
2. ä¼˜åŒ–å†…å­˜ç®¡ç†
3. æ·»åŠ æ€§èƒ½ç›‘æ§

### é˜¶æ®µ 2: æ·±åº¦ä¼˜åŒ– (1-2 å‘¨)
1. å®ç°æ‰¹é‡æ¨ç†
2. å¯ç”¨ torch.compile
3. ä¼˜åŒ– LoRA åˆ‡æ¢

### é˜¶æ®µ 3: é«˜çº§ä¼˜åŒ– (2-4 å‘¨)
1. CUDA Graph ä¼˜åŒ–
2. åŠ¨æ€æ‰¹å¤„ç†
3. è‡ªé€‚åº”èµ„æºåˆ†é…

## ğŸ”§ æµ‹è¯•éªŒè¯

ä½¿ç”¨ä¼˜åŒ–åˆ†æå·¥å…·éªŒè¯æ•ˆæœï¼š
```bash
# æµ‹è¯•å½“å‰æ€§èƒ½
python test_throughput_simple.py --requests 50 --concurrent 5

# æµ‹è¯•ä¼˜åŒ–åæ€§èƒ½
python test_throughput_optimized.py --requests 50 --concurrent 5
```

è¿™äº›ä¼˜åŒ–å¯ä»¥æ˜¾è‘—æå‡ FLUX API çš„ååé‡ï¼Œç‰¹åˆ«æ˜¯åœ¨é«˜å¹¶å‘åœºæ™¯ä¸‹ã€‚ 