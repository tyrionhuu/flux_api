# FLUX API Load Balancer Configuration - 2 GPUs
# Distributes requests across 2 GPU instances

# Set worker processes to auto (uses number of CPU cores)
worker_processes auto;

# Optimize for low latency
worker_rlimit_nofile 65535;

events {
    worker_connections 4096;
    use epoll;  # Efficient connection handling on Linux
    multi_accept on;
}

http {
    # Rate limiting zones - prevent overload
    # Reduced rate to prevent overwhelming the backends
    limit_req_zone $binary_remote_addr zone=api_limit:10m rate=10r/s;  
    limit_conn_zone $binary_remote_addr zone=addr_limit:10m;
    
    # Define upstream servers - one per GPU (2 GPUs)
    upstream flux_api_backend {
        # Use least connections algorithm for better load distribution
        least_conn;
        
        # Backends without max_conns limit - allows proper request queuing
        # Each backend will still process 1 request at a time internally
        server 127.0.0.1:23333 max_fails=3 fail_timeout=30s;
        server 127.0.0.1:23334 max_fails=3 fail_timeout=30s;

        # Enhanced keepalive connections for better performance
        keepalive 64;
        keepalive_requests 1000;
        keepalive_timeout 65s;
    }

    server {
    listen 8080;  # Listen on all interfaces
    listen [::]:8080;  # IPv6 support
    server_name _;  # Accept any hostname
    
    # Client body size limit for LoRA uploads (1GB)
    client_max_body_size 1024M;
    
    # Optimized timeouts for image generation
    proxy_connect_timeout 10s;   # Faster failure detection for connection issues
    proxy_send_timeout 600s;     # Keep high for actual request sending
    proxy_read_timeout 600s;     # Keep high for actual processing time
    send_timeout 600s;            # Keep high for response sending
    
    # Main API endpoint
    location / {
        # Rate limiting - burst allows temporary spikes but with delays
        # This queues requests at nginx level before sending to backends
        limit_req zone=api_limit burst=200 nodelay;  # Queue up to 200, process immediately
        limit_conn addr_limit 10;  # Max 10 concurrent connections per IP
        
        # Return 503 when backend is overloaded instead of hanging
        proxy_next_upstream error timeout http_503;
        proxy_next_upstream_tries 2;
        
        proxy_pass http://flux_api_backend;
        
        # Pass real client information
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
        
        # WebSocket support (if needed for streaming)
        proxy_http_version 1.1;
        proxy_set_header Upgrade $http_upgrade;
        proxy_set_header Connection "upgrade";
        
        # Enable buffering to queue requests at nginx level
        # This helps prevent overwhelming the backends
        proxy_buffering on;
        proxy_request_buffering on;
        proxy_buffer_size 128k;
        proxy_buffers 8 128k;
        proxy_busy_buffers_size 256k;
        
        # Keep connections alive properly
        proxy_set_header Connection "";
    }
    
    # Health check endpoint (nginx plus feature, or use external monitoring)
    location /health {
        access_log off;
        return 200 "healthy\n";
        add_header Content-Type text/plain;
    }
    
    # Status page (optional, requires nginx stub_status module)
    location /nginx_status {
        stub_status on;
        allow 127.0.0.1;
        deny all;
    }
}

    # Optional: SSL configuration
    # server {
    #     listen 443 ssl http2;
    #     server_name your-domain.com;
    #     
    #     ssl_certificate /path/to/cert.pem;
    #     ssl_certificate_key /path/to/key.pem;
    #     
    #     # ... rest of configuration same as above ...
    # }
}  # Close http block